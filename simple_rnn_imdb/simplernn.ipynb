{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end Deep Learning Project Using Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,SimpleRNN,Dense, Input\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # disable onednn for Nvidia training only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (25000,), Training labels shape: (25000,)\n",
      "Testing data shape: (25000,), Testing labels shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "## Load the imdb dataset\n",
    "\n",
    "vocabulary_size=10000 ##vocabulary size\n",
    "(X_train,y_train),(X_test,y_test)=imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f'Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}')\n",
    "print(f'Testing data shape: {X_train.shape}, Testing labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  14,\n",
       "  22,\n",
       "  16,\n",
       "  43,\n",
       "  530,\n",
       "  973,\n",
       "  1622,\n",
       "  1385,\n",
       "  65,\n",
       "  458,\n",
       "  4468,\n",
       "  66,\n",
       "  3941,\n",
       "  4,\n",
       "  173,\n",
       "  36,\n",
       "  256,\n",
       "  5,\n",
       "  25,\n",
       "  100,\n",
       "  43,\n",
       "  838,\n",
       "  112,\n",
       "  50,\n",
       "  670,\n",
       "  2,\n",
       "  9,\n",
       "  35,\n",
       "  480,\n",
       "  284,\n",
       "  5,\n",
       "  150,\n",
       "  4,\n",
       "  172,\n",
       "  112,\n",
       "  167,\n",
       "  2,\n",
       "  336,\n",
       "  385,\n",
       "  39,\n",
       "  4,\n",
       "  172,\n",
       "  4536,\n",
       "  1111,\n",
       "  17,\n",
       "  546,\n",
       "  38,\n",
       "  13,\n",
       "  447,\n",
       "  4,\n",
       "  192,\n",
       "  50,\n",
       "  16,\n",
       "  6,\n",
       "  147,\n",
       "  2025,\n",
       "  19,\n",
       "  14,\n",
       "  22,\n",
       "  4,\n",
       "  1920,\n",
       "  4613,\n",
       "  469,\n",
       "  4,\n",
       "  22,\n",
       "  71,\n",
       "  87,\n",
       "  12,\n",
       "  16,\n",
       "  43,\n",
       "  530,\n",
       "  38,\n",
       "  76,\n",
       "  15,\n",
       "  13,\n",
       "  1247,\n",
       "  4,\n",
       "  22,\n",
       "  17,\n",
       "  515,\n",
       "  17,\n",
       "  12,\n",
       "  16,\n",
       "  626,\n",
       "  18,\n",
       "  2,\n",
       "  5,\n",
       "  62,\n",
       "  386,\n",
       "  12,\n",
       "  8,\n",
       "  316,\n",
       "  8,\n",
       "  106,\n",
       "  5,\n",
       "  4,\n",
       "  2223,\n",
       "  5244,\n",
       "  16,\n",
       "  480,\n",
       "  66,\n",
       "  3785,\n",
       "  33,\n",
       "  4,\n",
       "  130,\n",
       "  12,\n",
       "  16,\n",
       "  38,\n",
       "  619,\n",
       "  5,\n",
       "  25,\n",
       "  124,\n",
       "  51,\n",
       "  36,\n",
       "  135,\n",
       "  48,\n",
       "  25,\n",
       "  1415,\n",
       "  33,\n",
       "  6,\n",
       "  22,\n",
       "  12,\n",
       "  215,\n",
       "  28,\n",
       "  77,\n",
       "  52,\n",
       "  5,\n",
       "  14,\n",
       "  407,\n",
       "  16,\n",
       "  82,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  107,\n",
       "  117,\n",
       "  5952,\n",
       "  15,\n",
       "  256,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  3766,\n",
       "  5,\n",
       "  723,\n",
       "  36,\n",
       "  71,\n",
       "  43,\n",
       "  530,\n",
       "  476,\n",
       "  26,\n",
       "  400,\n",
       "  317,\n",
       "  46,\n",
       "  7,\n",
       "  4,\n",
       "  2,\n",
       "  1029,\n",
       "  13,\n",
       "  104,\n",
       "  88,\n",
       "  4,\n",
       "  381,\n",
       "  15,\n",
       "  297,\n",
       "  98,\n",
       "  32,\n",
       "  2071,\n",
       "  56,\n",
       "  26,\n",
       "  141,\n",
       "  6,\n",
       "  194,\n",
       "  7486,\n",
       "  18,\n",
       "  4,\n",
       "  226,\n",
       "  22,\n",
       "  21,\n",
       "  134,\n",
       "  476,\n",
       "  26,\n",
       "  480,\n",
       "  5,\n",
       "  144,\n",
       "  30,\n",
       "  5535,\n",
       "  18,\n",
       "  51,\n",
       "  36,\n",
       "  28,\n",
       "  224,\n",
       "  92,\n",
       "  25,\n",
       "  104,\n",
       "  4,\n",
       "  226,\n",
       "  65,\n",
       "  16,\n",
       "  38,\n",
       "  1334,\n",
       "  88,\n",
       "  12,\n",
       "  16,\n",
       "  283,\n",
       "  5,\n",
       "  16,\n",
       "  4472,\n",
       "  113,\n",
       "  103,\n",
       "  32,\n",
       "  15,\n",
       "  16,\n",
       "  5345,\n",
       "  19,\n",
       "  178,\n",
       "  32],\n",
       " np.int64(1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review (as integers):[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "## Inspect a sample review and its label\n",
    "sample_review=X_train[0]\n",
    "sample_label=y_train[0]\n",
    "\n",
    "print(f\"Sample review (as integers):{sample_review}\")\n",
    "print(f'Sample label: {sample_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesn’t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitães',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'white’s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jōb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmé',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doña',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MApping of words index bacl to words(for understanding)\n",
    "word_index=imdb.get_word_index()\n",
    "#reverse the key to become value, and value to become key in the new dictionary\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From tensorflow documentation, the first 3 indices are reserved for padding, start of sequence and unknown\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in sample_review])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 3586,    2],\n",
       "       [   0,    0,    0, ...,   12,    9,   23],\n",
       "       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "## Pad the sequences to a fixed length of 500 words each sentence\n",
    "features_len=500\n",
    "\n",
    "X_train=sequence.pad_sequences(X_train,maxlen=features_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=features_len)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,   14,   22,   16,\n",
       "         43,  530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,\n",
       "        173,   36,  256,    5,   25,  100,   43,  838,  112,   50,  670,\n",
       "          2,    9,   35,  480,  284,    5,  150,    4,  172,  112,  167,\n",
       "          2,  336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,\n",
       "         13,  447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,\n",
       "         22,    4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,\n",
       "         43,  530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,\n",
       "         17,   12,   16,  626,   18,    2,    5,   62,  386,   12,    8,\n",
       "        316,    8,  106,    5,    4, 2223, 5244,   16,  480,   66, 3785,\n",
       "         33,    4,  130,   12,   16,   38,  619,    5,   25,  124,   51,\n",
       "         36,  135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,\n",
       "         77,   52,    5,   14,  407,   16,   82,    2,    8,    4,  107,\n",
       "        117, 5952,   15,  256,    4,    2,    7, 3766,    5,  723,   36,\n",
       "         71,   43,  530,  476,   26,  400,  317,   46,    7,    4,    2,\n",
       "       1029,   13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,\n",
       "         56,   26,  141,    6,  194, 7486,   18,    4,  226,   22,   21,\n",
       "        134,  476,   26,  480,    5,  144,   30, 5535,   18,   51,   36,\n",
       "         28,  224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,\n",
       "         88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,\n",
       "         16, 5345,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization, GRU\n",
    "\n",
    "## Train Simple RNN\n",
    "model=Sequential()\n",
    "dimensions=128 # Number of dimensions for the embedding layer,each word will be a vector of 128 dimensions\n",
    "neurons=256 # Number of neurons in the RNN layer\n",
    "# Add an Input layer to explicitly define the input shape, as array of 500 words as each input\n",
    "model.add(Input(shape=(features_len,)))\n",
    "# It may be more accurate to use gensim to train the word embeddings, and then load the embeddings into the model\n",
    "model.add(Embedding(vocabulary_size,dimensions,input_length=features_len)) ## Embedding Layers\n",
    "\n",
    "# return_sequences=True: returns the full sequence of outputs, allowing the second SimpleRNN layer to process the entire sequence\n",
    "# return_sequences is not needed for the last RNN layer, or if we just need 1 RNN layer\n",
    "model.add(SimpleRNN(neurons, activation='relu'))\n",
    "model.add(Dropout(0.1)) # 10% of the neurons will be dropped out randomly during training, to avoid overfitting\n",
    "# model.add(GRU(neurons, activation='relu'))  # GRU layer\n",
    "model.add(BatchNormalization()) # This may improve the training speed and stability\n",
    "\n",
    "model.add(Dense(1,activation=\"sigmoid\")) # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m98,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,136,453</span> (15.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,136,453\u001b[0m (15.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,378,817</span> (5.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,378,817\u001b[0m (5.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,636</span> (10.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,757,636\u001b[0m (10.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.early_stopping.EarlyStopping at 0x7f19d0ee7e90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an instance of EarlyStoppping Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Within 5 epochs, if the validation loss does not improve, stop the training and restore the best weights\n",
    "earlystopping=EarlyStopping(monitor='val_loss',patience=8,restore_best_weights=True)\n",
    "earlystopping\n",
    "# If the validation loss does not improve after 5 epochs, reduce the learning rate by 20%\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.6433 - loss: 9284893016064.0000 - val_accuracy: 0.5528 - val_loss: 0.7641\n",
      "Epoch 2/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 47ms/step - accuracy: 0.5545 - loss: 0.7657 - val_accuracy: 0.5570 - val_loss: 0.7536\n",
      "Epoch 3/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 47ms/step - accuracy: 0.5870 - loss: 0.6599 - val_accuracy: 0.5906 - val_loss: 0.6778\n",
      "Epoch 4/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.6853 - loss: 0.5469 - val_accuracy: 0.6874 - val_loss: 0.6573\n",
      "Epoch 5/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.6836 - loss: 0.5368 - val_accuracy: 0.6230 - val_loss: 0.6611\n",
      "Epoch 6/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.7537 - loss: 0.4778 - val_accuracy: 0.6984 - val_loss: 0.6077\n",
      "Epoch 7/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8471 - loss: 0.3636 - val_accuracy: 0.7754 - val_loss: 0.5673\n",
      "Epoch 8/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8550 - loss: 0.3259 - val_accuracy: 0.7728 - val_loss: 0.5822\n",
      "Epoch 9/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.9019 - loss: 0.2442 - val_accuracy: 0.7722 - val_loss: 0.6138\n",
      "Epoch 10/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.9171 - loss: 0.2128 - val_accuracy: 0.7638 - val_loss: 0.6559\n",
      "Epoch 11/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.9296 - loss: 0.1820 - val_accuracy: 0.7754 - val_loss: 0.6272\n",
      "Epoch 12/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.9426 - loss: 0.1491 - val_accuracy: 0.7904 - val_loss: 0.7458\n",
      "Epoch 13/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.9351 - loss: 0.1647 - val_accuracy: 0.7874 - val_loss: 0.6783\n",
      "Epoch 14/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.9487 - loss: 0.1351 - val_accuracy: 0.7832 - val_loss: 0.7406\n",
      "Epoch 15/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.9604 - loss: 0.1038 - val_accuracy: 0.7712 - val_loss: 0.7455\n"
     ]
    }
   ],
   "source": [
    "## Train the model with early sstopping\n",
    "## Using each batch of 32 samples, and find the best weights for each batch. Total we have 25000 samples / 64 = 390 batches\n",
    "## Using batch would reduce memory comsumption and avoid finding the wrong descent, \n",
    "## as there could be multiple of local minimums (local descent), and we only\n",
    "## want to find the global minimum (global descent)\n",
    "## validation_split=0.2, means 20% of the training data will be used as validation data\n",
    "history=model.fit(\n",
    "    X_train,y_train,epochs=20,batch_size=64, # increase the number of epochs or batch_size will not add more training inputs\n",
    "    validation_split=0.2,\n",
    "    callbacks=[earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.2565163 , -0.57055885,  0.5363279 , ..., -0.18752316,\n",
       "          0.6912675 ,  0.44828653],\n",
       "        [-0.02807593, -0.00598582,  0.00481732, ..., -0.0573726 ,\n",
       "         -0.04008389, -0.01020903],\n",
       "        [ 0.03349014, -0.07547865,  0.06802489, ...,  0.0105071 ,\n",
       "          0.01539172,  0.03550284],\n",
       "        ...,\n",
       "        [ 0.01798494,  0.01533521,  0.03510292, ..., -0.08273786,\n",
       "          0.02954749, -0.06794201],\n",
       "        [-0.26360333, -0.15151593, -0.19742312, ..., -0.13173084,\n",
       "          0.19870712,  0.25914222],\n",
       "        [-0.2636011 ,  0.08350825, -0.2180802 , ...,  0.27248892,\n",
       "         -0.4151401 , -0.4099179 ]], dtype=float32),\n",
       " array([[-0.00640702, -0.11374757, -0.08311329, ..., -0.01042442,\n",
       "          0.06875041,  0.00414912],\n",
       "        [-0.05087542,  0.09958373,  0.06352261, ..., -0.01644837,\n",
       "          0.07035819, -0.01857858],\n",
       "        [-0.10265442,  0.09974295, -0.02662472, ..., -0.09273013,\n",
       "          0.0544079 , -0.04918066],\n",
       "        ...,\n",
       "        [-0.12559566,  0.02282128, -0.01448064, ..., -0.10078923,\n",
       "          0.06815921, -0.00898471],\n",
       "        [ 0.12239961,  0.05769316, -0.03730138, ..., -0.02498229,\n",
       "         -0.10990523,  0.08931489],\n",
       "        [ 0.09116897, -0.00386363,  0.03506765, ..., -0.03711227,\n",
       "         -0.12690109,  0.07200261]], dtype=float32),\n",
       " array([[ 0.00801042,  0.02318718, -0.17403816, ..., -0.00677672,\n",
       "         -0.04679832,  0.02799702],\n",
       "        [-0.1088423 , -0.07818711, -0.05012036, ..., -0.07035521,\n",
       "         -0.01409816,  0.04927982],\n",
       "        [ 0.06749173,  0.03751751,  0.01937456, ...,  0.03714855,\n",
       "          0.03479451,  0.08709649],\n",
       "        ...,\n",
       "        [-0.1215832 ,  0.11251142,  0.06539759, ..., -0.04131486,\n",
       "         -0.01115699,  0.14213352],\n",
       "        [-0.10717019, -0.00432345,  0.02316931, ...,  0.02602797,\n",
       "          0.05815922, -0.0370726 ],\n",
       "        [ 0.06751287, -0.05181037, -0.03114537, ...,  0.01617658,\n",
       "         -0.09171536,  0.05666992]], dtype=float32),\n",
       " array([-0.01762844, -0.00738062, -0.01605967, -0.00037361, -0.00296233,\n",
       "        -0.02440372,  0.01139028, -0.01668075, -0.01867943, -0.02845604,\n",
       "        -0.02368364, -0.01288417, -0.03920924, -0.00784523,  0.00232195,\n",
       "        -0.0045601 , -0.03773572,  0.01251437, -0.02349307, -0.01554579,\n",
       "        -0.01842229, -0.01849089, -0.01679969, -0.02007977, -0.00878213,\n",
       "        -0.0114958 , -0.00462864, -0.00617954, -0.01438604, -0.01732158,\n",
       "        -0.00440648, -0.02419422,  0.0067051 , -0.01538404,  0.00085117,\n",
       "         0.00850007,  0.005021  , -0.02111891,  0.02074697, -0.01739829,\n",
       "        -0.00933766, -0.01363967, -0.03487164, -0.0135899 ,  0.01292793,\n",
       "        -0.00236069, -0.00661289, -0.02444233, -0.00784091,  0.0243284 ,\n",
       "        -0.01208691, -0.02523407, -0.0100897 , -0.0161354 , -0.00554538,\n",
       "        -0.02008468, -0.00849586, -0.01440991,  0.00049553,  0.00103116,\n",
       "        -0.00092192, -0.0223375 ,  0.00951913, -0.00856692, -0.00158509,\n",
       "         0.0240855 , -0.01664343, -0.0046288 , -0.02465272, -0.02456412,\n",
       "         0.00121294,  0.0104936 , -0.01054811, -0.00393499,  0.00209398,\n",
       "        -0.00258681,  0.00426156,  0.02088061, -0.00866676, -0.01498097,\n",
       "        -0.01728882, -0.00781612, -0.00559243,  0.0079712 , -0.00312551,\n",
       "        -0.01317099, -0.02699928,  0.0045402 , -0.01610349, -0.01484201,\n",
       "        -0.02173288, -0.02436495, -0.00667722,  0.00205556,  0.00765281,\n",
       "        -0.00414635,  0.00124539, -0.02438646,  0.01683618,  0.00953503,\n",
       "        -0.01282176, -0.00149422, -0.00784454,  0.01886382,  0.011105  ,\n",
       "        -0.0275245 , -0.00886568,  0.00507401, -0.0234967 , -0.0082888 ,\n",
       "         0.00967478,  0.00931324, -0.00295026, -0.02213406, -0.00694049,\n",
       "        -0.02742389, -0.01830197,  0.00088792, -0.0198979 , -0.0091113 ,\n",
       "        -0.01200095, -0.00493753,  0.00552994, -0.01016141,  0.01118871,\n",
       "         0.03055738, -0.00673814, -0.0030543 , -0.00984237,  0.00993544,\n",
       "         0.00478848,  0.00077177, -0.00124909,  0.00017933,  0.0193933 ,\n",
       "        -0.02195685, -0.00222108, -0.01043184, -0.00421627, -0.00183468,\n",
       "         0.01699875,  0.01062808,  0.00284646,  0.00023695,  0.00342366,\n",
       "         0.00388583, -0.00440156, -0.03128444,  0.00490008, -0.00062627,\n",
       "         0.01222016, -0.02691241,  0.00126961, -0.01811035, -0.00152054,\n",
       "        -0.00679874,  0.01363833, -0.01476286,  0.00830804, -0.00687906,\n",
       "        -0.01165611, -0.00308189, -0.04161174,  0.01454463, -0.00226119,\n",
       "        -0.01753442, -0.00320594, -0.02818425,  0.00597022, -0.01847996,\n",
       "        -0.0299928 ,  0.01446649, -0.04011041, -0.02155509, -0.01577499,\n",
       "        -0.03299279, -0.02889832,  0.00032623, -0.02624277, -0.00480504,\n",
       "        -0.00585518, -0.00893286, -0.03081917, -0.01848863,  0.01470656,\n",
       "         0.00948257, -0.02478366, -0.02293768,  0.02932829, -0.00790398,\n",
       "        -0.01336724, -0.00941733, -0.02160773, -0.00709989, -0.02295806,\n",
       "        -0.01172322, -0.00581703, -0.02167908, -0.01286268, -0.0026867 ,\n",
       "        -0.0110949 , -0.00921668, -0.02460022, -0.02328499, -0.00486293,\n",
       "        -0.00286841, -0.02207345, -0.0083541 , -0.02027462,  0.01843287,\n",
       "        -0.0230195 , -0.04418699,  0.01013537, -0.00074146, -0.02654219,\n",
       "        -0.02598115, -0.0017368 , -0.00153836, -0.00495461, -0.01106676,\n",
       "         0.013083  , -0.02429476, -0.03973735,  0.00137126,  0.00106506,\n",
       "        -0.03281753,  0.02802199,  0.01159312, -0.00758976, -0.01203302,\n",
       "        -0.02976591,  0.00322072, -0.01623416, -0.02989576, -0.00363204,\n",
       "        -0.02183512, -0.02319816, -0.01304983, -0.01271372, -0.03472411,\n",
       "         0.00234728, -0.00499784, -0.03159847, -0.00393917,  0.0050828 ,\n",
       "        -0.0057109 , -0.01156664, -0.02181888, -0.02585724,  0.00048287,\n",
       "        -0.01501249, -0.00428627,  0.01280395, -0.02762141, -0.01531939,\n",
       "        -0.01404951], dtype=float32),\n",
       " array([[-8.73907879e-02],\n",
       "        [-9.01986808e-02],\n",
       "        [ 3.48700560e-03],\n",
       "        [-7.74134099e-02],\n",
       "        [-9.46767107e-02],\n",
       "        [-9.94660612e-03],\n",
       "        [-8.28855559e-02],\n",
       "        [ 1.19694620e-02],\n",
       "        [ 1.07602272e-02],\n",
       "        [ 9.99288931e-02],\n",
       "        [ 1.70139834e-01],\n",
       "        [ 7.03057274e-02],\n",
       "        [ 3.19649291e-04],\n",
       "        [ 7.67612308e-02],\n",
       "        [-4.46341597e-02],\n",
       "        [-1.27194878e-02],\n",
       "        [ 2.04390939e-02],\n",
       "        [-7.98567235e-02],\n",
       "        [ 5.15200913e-01],\n",
       "        [-8.57673138e-02],\n",
       "        [ 2.56778747e-01],\n",
       "        [ 3.19780596e-02],\n",
       "        [-1.07861750e-01],\n",
       "        [-7.12156221e-02],\n",
       "        [ 7.68549293e-02],\n",
       "        [ 6.62976727e-02],\n",
       "        [-1.18533485e-01],\n",
       "        [ 1.12023801e-01],\n",
       "        [ 3.06931645e-01],\n",
       "        [ 2.04840750e-02],\n",
       "        [ 4.37139459e-02],\n",
       "        [ 2.27380142e-01],\n",
       "        [ 1.15460187e-01],\n",
       "        [-8.28233659e-02],\n",
       "        [ 1.91472277e-01],\n",
       "        [-4.44038026e-02],\n",
       "        [-2.25512594e-01],\n",
       "        [ 6.47490501e-01],\n",
       "        [-7.34056085e-02],\n",
       "        [-1.10593900e-01],\n",
       "        [ 4.15658876e-02],\n",
       "        [-7.16734827e-02],\n",
       "        [ 1.09485745e-01],\n",
       "        [-1.73612165e+00],\n",
       "        [ 1.88075081e-01],\n",
       "        [-7.70474970e-02],\n",
       "        [ 1.47475779e-01],\n",
       "        [ 2.20520079e-01],\n",
       "        [ 8.02548155e-02],\n",
       "        [-1.01184277e-02],\n",
       "        [-3.57891023e-02],\n",
       "        [ 1.10885631e-02],\n",
       "        [ 1.10923527e-02],\n",
       "        [ 2.56439000e-01],\n",
       "        [-1.73806086e-01],\n",
       "        [ 5.90054512e-01],\n",
       "        [ 1.51159570e-01],\n",
       "        [-1.41703725e-01],\n",
       "        [ 8.50796625e-02],\n",
       "        [ 1.47466630e-01],\n",
       "        [ 8.88214931e-02],\n",
       "        [-2.23822705e-02],\n",
       "        [-4.10218120e-01],\n",
       "        [ 6.17236555e-01],\n",
       "        [-1.01797640e-01],\n",
       "        [-6.47446513e-02],\n",
       "        [ 9.58813280e-02],\n",
       "        [ 1.73128806e-02],\n",
       "        [ 1.54639676e-01],\n",
       "        [ 1.63029045e-01],\n",
       "        [-2.44714469e-02],\n",
       "        [-1.09573305e-01],\n",
       "        [ 1.56453699e-01],\n",
       "        [-1.34496376e-01],\n",
       "        [-1.45226205e-02],\n",
       "        [-9.25302282e-02],\n",
       "        [-2.17391297e-01],\n",
       "        [ 1.02621429e-01],\n",
       "        [ 7.44821653e-02],\n",
       "        [ 6.82926923e-02],\n",
       "        [-6.68700039e-02],\n",
       "        [-2.30347347e-02],\n",
       "        [ 7.18634069e-01],\n",
       "        [ 1.20131768e-01],\n",
       "        [-3.63876879e-01],\n",
       "        [ 9.70299635e-03],\n",
       "        [-8.01035464e-02],\n",
       "        [-4.95266207e-02],\n",
       "        [ 3.19957547e-02],\n",
       "        [ 8.84130225e-03],\n",
       "        [ 9.43490043e-02],\n",
       "        [ 2.83348057e-02],\n",
       "        [-2.45719075e+00],\n",
       "        [-1.67661220e-01],\n",
       "        [ 5.58377467e-02],\n",
       "        [ 5.11923075e-01],\n",
       "        [-1.11088797e-01],\n",
       "        [ 1.84174493e-01],\n",
       "        [ 3.82207595e-02],\n",
       "        [ 1.38363242e-01],\n",
       "        [ 8.72083083e-02],\n",
       "        [ 4.96270321e-02],\n",
       "        [-1.39472947e-01],\n",
       "        [ 3.22638094e-01],\n",
       "        [-3.20915103e-01],\n",
       "        [ 1.92875583e-02],\n",
       "        [-1.25498176e+00],\n",
       "        [ 2.99030215e-01],\n",
       "        [ 2.80504934e-02],\n",
       "        [-4.42264646e-01],\n",
       "        [ 7.40635753e-01],\n",
       "        [ 1.08051086e-02],\n",
       "        [ 5.95851690e-02],\n",
       "        [ 2.61712283e-01],\n",
       "        [ 1.81881011e-01],\n",
       "        [ 1.19065531e-01],\n",
       "        [ 1.60855815e-01],\n",
       "        [ 2.91867368e-02],\n",
       "        [-5.80284651e-03],\n",
       "        [ 8.66165906e-02],\n",
       "        [-3.40740681e-02],\n",
       "        [-7.48087615e-02],\n",
       "        [ 2.19482169e-01],\n",
       "        [-2.95467097e-02],\n",
       "        [-7.05932304e-02],\n",
       "        [-1.06901690e-01],\n",
       "        [ 4.58357036e-02],\n",
       "        [-1.34947464e-01],\n",
       "        [ 7.20755160e-02],\n",
       "        [-1.43744973e-02],\n",
       "        [-4.91220839e-02],\n",
       "        [-2.22481191e-02],\n",
       "        [-6.45734668e-02],\n",
       "        [-2.17551272e-02],\n",
       "        [-1.06856532e-01],\n",
       "        [-4.70478058e-01],\n",
       "        [-1.24965653e-01],\n",
       "        [-1.14479113e+00],\n",
       "        [ 2.93012798e-01],\n",
       "        [-1.47883566e-02],\n",
       "        [ 1.14048598e-02],\n",
       "        [ 7.44500905e-02],\n",
       "        [ 9.77007672e-02],\n",
       "        [-8.72870460e-02],\n",
       "        [-2.48407856e-01],\n",
       "        [ 2.31648441e-02],\n",
       "        [-7.65413642e-02],\n",
       "        [ 1.77486494e-01],\n",
       "        [ 1.76104501e-01],\n",
       "        [ 1.03242956e-01],\n",
       "        [-5.38398400e-02],\n",
       "        [-6.34473935e-02],\n",
       "        [-1.05598010e-01],\n",
       "        [-1.83915019e-01],\n",
       "        [-7.71770403e-02],\n",
       "        [-1.66772127e-01],\n",
       "        [ 9.13112983e-02],\n",
       "        [ 2.05766231e-01],\n",
       "        [ 4.61890437e-02],\n",
       "        [ 1.11853518e-01],\n",
       "        [-7.64289647e-02],\n",
       "        [ 1.94092304e-01],\n",
       "        [-7.15279207e-02],\n",
       "        [-1.35119349e-01],\n",
       "        [ 4.35791075e-01],\n",
       "        [ 8.12128857e-02],\n",
       "        [-4.32277024e-02],\n",
       "        [ 6.44828379e-02],\n",
       "        [-9.49568078e-02],\n",
       "        [ 1.57172773e-02],\n",
       "        [ 8.29795450e-02],\n",
       "        [-7.34363794e-02],\n",
       "        [ 1.92041636e-01],\n",
       "        [-6.07349314e-02],\n",
       "        [ 6.64343610e-02],\n",
       "        [ 7.62525499e-01],\n",
       "        [ 2.39719804e-02],\n",
       "        [-3.17729414e-01],\n",
       "        [ 5.47520630e-02],\n",
       "        [ 1.17781185e-01],\n",
       "        [-4.25959267e-02],\n",
       "        [-3.42684127e-02],\n",
       "        [ 6.98256642e-02],\n",
       "        [-1.88174080e-02],\n",
       "        [-4.10752632e-02],\n",
       "        [-4.04988602e-02],\n",
       "        [ 1.36090323e-01],\n",
       "        [ 7.28697553e-02],\n",
       "        [ 3.03396527e-02],\n",
       "        [ 9.00773481e-02],\n",
       "        [ 9.57306921e-02],\n",
       "        [-3.55569012e-02],\n",
       "        [ 1.66568458e-01],\n",
       "        [-8.77903122e-03],\n",
       "        [-2.94557363e-01],\n",
       "        [ 6.53320774e-02],\n",
       "        [ 8.68834555e-01],\n",
       "        [-1.71236798e-01],\n",
       "        [-2.84134373e-02],\n",
       "        [ 6.60791397e-02],\n",
       "        [ 3.86313915e-01],\n",
       "        [ 6.33996353e-02],\n",
       "        [-1.29940644e-01],\n",
       "        [-5.38949072e-02],\n",
       "        [-1.40155748e-01],\n",
       "        [-5.42390585e-01],\n",
       "        [ 2.04883236e-02],\n",
       "        [-1.64221555e-01],\n",
       "        [ 7.30696395e-02],\n",
       "        [-5.60232215e-02],\n",
       "        [ 3.94218683e-01],\n",
       "        [-7.94156045e-02],\n",
       "        [ 3.63475122e-02],\n",
       "        [-3.21426578e-02],\n",
       "        [ 7.88445622e-02],\n",
       "        [-3.14167261e-01],\n",
       "        [ 1.20029859e-01],\n",
       "        [ 7.34186843e-02],\n",
       "        [-9.30720642e-02],\n",
       "        [ 4.66471463e-02],\n",
       "        [-6.64268211e-02],\n",
       "        [ 5.99896796e-02],\n",
       "        [-6.15947664e-01],\n",
       "        [-1.68063915e+00],\n",
       "        [-1.44818783e-01],\n",
       "        [ 2.72520453e-01],\n",
       "        [-1.12046748e-01],\n",
       "        [ 6.99665487e-01],\n",
       "        [ 1.08240090e-01],\n",
       "        [ 8.24608430e-02],\n",
       "        [ 7.27414638e-02],\n",
       "        [ 9.04958472e-02],\n",
       "        [-8.81648585e-02],\n",
       "        [-2.28105888e-01],\n",
       "        [-1.29590869e-01],\n",
       "        [ 6.02678433e-02],\n",
       "        [ 1.49632543e-02],\n",
       "        [-4.71227579e-02],\n",
       "        [-6.62818104e-02],\n",
       "        [ 3.16127539e-01],\n",
       "        [-3.42968828e-03],\n",
       "        [ 2.86472831e-02],\n",
       "        [-1.40629224e-02],\n",
       "        [-9.02292132e-02],\n",
       "        [ 2.23609973e-02],\n",
       "        [ 1.01875357e-01],\n",
       "        [ 7.05549717e-02],\n",
       "        [-1.66265595e+00],\n",
       "        [ 5.84790170e-01],\n",
       "        [ 1.87734798e-01],\n",
       "        [-5.87300025e-02],\n",
       "        [-4.73190367e-01],\n",
       "        [-1.17839932e-01],\n",
       "        [ 1.86329469e-01],\n",
       "        [ 8.50917771e-02],\n",
       "        [-4.66517843e-02]], dtype=float32),\n",
       " array([-0.30277315], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "## Save model file\n",
    "model.save('simple_rnn_imdb.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
