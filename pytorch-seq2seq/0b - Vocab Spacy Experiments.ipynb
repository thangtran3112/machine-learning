{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load spaCy models\n",
    "try:\n",
    "    en_nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "    de_nlp = spacy.load('de_core_news_sm', disable=['parser', 'tagger', 'ner'])\n",
    "except OSError:\n",
    "    # Download if not present\n",
    "    import os\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    os.system('python -m spacy download de_core_news_sm')\n",
    "    en_nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "    de_nlp = spacy.load('de_core_news_sm', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "data = [\n",
    "    {\n",
    "        'en': 'The cat and dog are playing.',\n",
    "        'de': 'Die Katze und der Hund spielen.',\n",
    "        'en_tokens': ['<sos>', 'the', 'cat', 'and', 'dog', 'are', 'playing', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'die', 'katze', 'und', 'der', 'hund', 'spielen', '.', '<eos>']\n",
    "    },\n",
    "    {\n",
    "        'en': 'The cat and cat are sleeping.',\n",
    "        'de': 'Die Katze und Katze schlafen.',\n",
    "        'en_tokens': ['<sos>', 'the', 'cat', 'and', 'cat', 'are', 'sleeping', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'die', 'katze', 'und', 'katze', 'schlafen', '.', '<eos>']\n",
    "    },\n",
    "    {\n",
    "        'en': 'The dog is running.',\n",
    "        'de': 'Der Hund läuft.',\n",
    "        'en_tokens': ['<sos>', 'the', 'dog', 'is', 'running', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'der', 'hund', 'läuft', '.', '<eos>']\n",
    "    },\n",
    "    {\n",
    "        'en': 'Two young, White males are outside near many bushes.',\n",
    "        'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
    "        'en_tokens': ['<sos>', 'two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
    "    },\n",
    "    {\n",
    "        'en': 'Two young people are walking near bushes.',\n",
    "        'de': 'Zwei junge Leute gehen in der Nähe von Büschen.',\n",
    "        'en_tokens': ['<sos>', 'two', 'young', 'people', 'are', 'walking', 'near', 'bushes', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'zwei', 'junge', 'leute', 'gehen', 'in', 'der', 'nähe', 'von', 'büschen', '.', '<eos>']\n",
    "    },\n",
    "    {\n",
    "        'en': 'The young students are outside.',\n",
    "        'de': 'Die junge Studenten sind im Freien.',\n",
    "        'en_tokens': ['<sos>', 'the', 'young', 'students', 'are', 'outside', '.', '<eos>'],\n",
    "        'de_tokens': ['<sos>', 'die', 'junge', 'studenten', 'sind', 'im', 'freien', '.', '<eos>']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 15\n",
      "German vocab size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thangtran3112/machine-learning/pytorch-seq2seq/venv/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "def build_vocab_from_parallel_data(data_iterator: List[Dict], \n",
    "                                 min_freq: int = 2, \n",
    "                                 specials: List[str] = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]):\n",
    "    \"\"\"Build vocabularies from parallel tokenized data using spaCy\"\"\"\n",
    "    # Initialize counters\n",
    "    en_counter = Counter()\n",
    "    de_counter = Counter()\n",
    "    \n",
    "    # Count tokens using spaCy tokenization\n",
    "    for item in data_iterator:\n",
    "        en_tokens = [token.text.lower() for token in en_nlp(item['en'])]\n",
    "        de_tokens = [token.text.lower() for token in de_nlp(item['de'])]\n",
    "        en_counter.update(en_tokens)\n",
    "        de_counter.update(de_tokens)\n",
    "    \n",
    "    def create_vocab(counter):\n",
    "        vocab = {}\n",
    "        # Add special tokens\n",
    "        for i, token in enumerate(specials):\n",
    "            vocab[token] = i\n",
    "        \n",
    "        # Add frequent tokens\n",
    "        idx = len(specials)\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq and token not in vocab:\n",
    "                vocab[token] = idx\n",
    "                idx += 1\n",
    "        return vocab\n",
    "    \n",
    "    en_vocab = create_vocab(en_counter)\n",
    "    de_vocab = create_vocab(de_counter)\n",
    "    \n",
    "    return en_vocab, de_vocab, en_counter, de_counter\n",
    "\n",
    "en_vocab, de_vocab, en_counter, de_counter = build_vocab_from_parallel_data(data)\n",
    "print(\"English vocab size:\", len(en_vocab))\n",
    "print(\"German vocab size:\", len(de_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab: {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, '.': 4, 'are': 5, 'the': 6, 'cat': 7, 'young': 8, 'and': 9, 'dog': 10, 'two': 11, 'outside': 12, 'near': 13, 'bushes': 14}\n",
      "English vocab: {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, '.': 4, 'der': 5, 'die': 6, 'katze': 7, 'junge': 8, 'und': 9, 'hund': 10, 'zwei': 11, 'sind': 12, 'im': 13, 'freien': 14, 'in': 15, 'nähe': 16}\n"
     ]
    }
   ],
   "source": [
    "# Display english vocab\n",
    "print(\"English vocab:\", en_vocab)\n",
    "\n",
    "# Display german vocab\n",
    "print(\"English vocab:\", de_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above function, we can generalize with `set_default_index` when the given word is unknown (`<unk>`)\n",
    "- `lookup_indices` will return a list of corresponding indexes, given an input list of string\n",
    "- `lookup_tokens` will return a list of corresponding vocab words, given an input list of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab : {0: '<unk>', 1: '<pad>', 2: '<sos>', 3: '<eos>', 4: '.', 5: 'are', 6: 'the', 7: 'cat', 8: 'young', 9: 'and', 10: 'dog', 11: 'two', 12: 'outside', 13: 'near', 14: 'bushes'}\n",
      "German vocab size: {0: '<unk>', 1: '<pad>', 2: '<sos>', 3: '<eos>', 4: '.', 5: 'der', 6: 'die', 7: 'katze', 8: 'junge', 9: 'und', 10: 'hund', 11: 'zwei', 12: 'sind', 13: 'im', 14: 'freien', 15: 'in', 16: 'nähe'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens_to_index):\n",
    "        self.tokens_to_index = tokens_to_index\n",
    "        self.index_to_tokens = {v: k for k, v in tokens_to_index.items()}\n",
    "        self.default_index = None\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"Retrieve index of token\"\"\"\n",
    "        return self.tokens_to_index.get(token, self.default_index)\n",
    "    \n",
    "    # We can run `\"the\" in en_vocab` to check if a token is in the vocabulary\n",
    "    def __contains__(self, token):\n",
    "        \"\"\"Enable membership testing with 'in' operator\"\"\"\n",
    "        return (token in self.tokens_to_index)\n",
    "    \n",
    "    def set_default_index(self, index):\n",
    "        self.default_index = index\n",
    "        \n",
    "    def lookup_indices(self, tokens):\n",
    "        \"\"\"Convert a list of tokens to indices\"\"\"\n",
    "        return [self.tokens_to_index.get(token, self.default_index) for token in tokens]\n",
    "    \n",
    "    def lookup_tokens(self, indices):\n",
    "        \"\"\"Convert a list of indices back to tokens\"\"\"\n",
    "        return [self.index_to_tokens.get(idx, self.index_to_tokens[self.default_index]) \n",
    "                for idx in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens_to_index)\n",
    "\n",
    "def build_vocab_from_parallel_data(data_iterator: List[Dict], \n",
    "                                 min_freq: int = 2, \n",
    "                                 specials: List[str] = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]):\n",
    "    \"\"\"Build vocabularies from parallel tokenized data using spaCy\"\"\"\n",
    "    en_counter = Counter()\n",
    "    de_counter = Counter()\n",
    "    \n",
    "    for item in data_iterator:\n",
    "        en_tokens = [token.text.lower() for token in en_nlp(item['en'])]\n",
    "        de_tokens = [token.text.lower() for token in de_nlp(item['de'])]\n",
    "        en_counter.update(en_tokens)\n",
    "        de_counter.update(de_tokens)\n",
    "    \n",
    "    def create_vocab(counter):\n",
    "        tokens_to_index = {}\n",
    "        for i, token in enumerate(specials):\n",
    "            tokens_to_index[token] = i\n",
    "        \n",
    "        idx = len(specials)\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq and token not in tokens_to_index:\n",
    "                tokens_to_index[token] = idx\n",
    "                idx += 1\n",
    "                \n",
    "        vocab = Vocabulary(tokens_to_index)\n",
    "        vocab.set_default_index(tokens_to_index[\"<unk>\"])\n",
    "        return vocab\n",
    "    \n",
    "    en_vocab = create_vocab(en_counter)\n",
    "    de_vocab = create_vocab(de_counter)\n",
    "    \n",
    "    return en_vocab, de_vocab, en_counter, de_counter\n",
    "\n",
    "# Build vocabularies\n",
    "en_vocab, de_vocab, en_counter, de_counter = build_vocab_from_parallel_data(data)\n",
    "\n",
    "# Test vocabulary\n",
    "print(\"English vocab :\", en_vocab.index_to_tokens)\n",
    "print(\"German vocab size:\", de_vocab.index_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"unknown word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing lookup_indices, upper-case 'Two' is considered as unknown word\n",
    "# We may handle lower case directly in the build_vocab_from_parallel_data function, but we assume the input is already lower-cased\n",
    "indices = en_vocab.lookup_indices(['Two', 'young', 'unknownword'])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'young', '<unk>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing lookup_tokens\n",
    "tokens = en_vocab.lookup_tokens(indices)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing __contains__\n",
    "\"the\" in en_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
