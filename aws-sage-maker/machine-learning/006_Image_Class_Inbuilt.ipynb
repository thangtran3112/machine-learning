{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ceed41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%load_ext nb_black\";\n                var nbb_formatted_code = \"%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c88b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"import os\\n\\n#!/bin/bash\\nif not os.path.exists('./images/flowers.zip') and not os.path.exists('./images/flowers'):\\n  !curl -L -o ./images/flowers.zip https://www.kaggle.com/api/v1/datasets/download/alxmamaev/flowers-recognition\\n  !unzip -o ./images/flowers.zip -d ./images\\n  !rm ./images/flowers.zip\\nelse:\\n  print(\\\"Dataset already downloaded\\\")\";\n                var nbb_formatted_code = \"import os\\n\\n#!/bin/bash\\nif not os.path.exists(\\\"./images/flowers.zip\\\") and not os.path.exists(\\n    \\\"./images/flowers\\\"\\n):\\n    !curl -L -o ./images/flowers.zip https://www.kaggle.com/api/v1/datasets/download/alxmamaev/flowers-recognition\\n    !unzip -o ./images/flowers.zip -d ./images\\n    !rm ./images/flowers.zip\\nelse:\\n    print(\\\"Dataset already downloaded\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#!/bin/bash\n",
    "if not os.path.exists('./images/flowers'):\n",
    "  !curl -L -o ./images/flowers.zip https://www.kaggle.com/api/v1/datasets/download/alxmamaev/flowers-recognition\n",
    "  !unzip -o ./images/flowers.zip -d ./images\n",
    "  !rm ./images/flowers.zip\n",
    "else:\n",
    "  print(\"Dataset already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7926d",
   "metadata": {},
   "source": [
    "* Now we will copy 30 (CLASS_SAMPLES number of images) first *.jpg files from each subfolder of [daisy, rose, dandelion] into [data/sgmkr_clf_lst/train_imgs](./data/sgmkr_clf_lst/train_imgs)\n",
    "* While copying each file, we will add a line into [data/sgmkr_clf_lst/train_annots/train.lst](./data/sgmkr_clf_lst/train_annots/train.lst), in the format such as [line_number, classification, file_name]. \n",
    "* Corresponding classifications of [daisy, rose, dandelion] are [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f039277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images copied successfully.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"import os\\nimport shutil\\ndata_path = \\\"classifications\\\"\\n\\n# List of subfolders\\nsubfolders = ['daisy', 'rose', 'dandelion']\\n\\nCLASS_SAMPLES = 30\\n\\n# Define the source and destination directories\\nsource_dir = './images/flowers'\\ntrain_img_dir = f'./data/{data_path}/train_imgs'\\n\\ntrain_lst_dir = f'./data/{data_path}/train_annots/train.lst'\\n\\n# Check if the destination directory exists and contains files\\nif os.path.exists(train_img_dir) and any(os.scandir(train_img_dir)):\\n  print(\\\"Destination directory already exists and contains files. Skipping copy.\\\")\\nelse:\\n  # Create the destination directory if it doesn't exist\\n  os.makedirs(train_img_dir, exist_ok=True)\\n\\n  # Create a new train.lst file, and remove the old one if it exists\\n  if os.path.exists(train_lst_dir):\\n    os.remove(train_lst_dir)\\n  os.makedirs(os.path.dirname(train_lst_dir), exist_ok=True)\\n\\n  with open(train_lst_dir, 'w') as train_lst_file:\\n    # Copy the first CLASS_SAMPLES number of images from each subfolder\\n    line_number = 1\\n    for subfolder in subfolders:\\n      subfolder_path = os.path.join(source_dir, subfolder)\\n      destination_subfolder_path = train_img_dir\\n      os.makedirs(destination_subfolder_path, exist_ok=True)\\n\\n      # Get the list of image files in the subfolder\\n      image_files = [f for f in os.listdir(subfolder_path) if f.endswith('.jpg')]\\n\\n      # Copy the first CLASS_SAMPLES number of images\\n      classification_number = subfolders.index(subfolder)\\n      for image_file in image_files[:CLASS_SAMPLES]:\\n        src_file = os.path.join(subfolder_path, image_file)\\n        dst_file = os.path.join(destination_subfolder_path, image_file)\\n        shutil.copy(src_file, dst_file)\\n        # Write to train.lst file\\n        train_lst_file.write(f\\\"{line_number}\\\\t{classification_number}\\\\t{image_file}\\\\n\\\")\\n        line_number += 1\\n  print(\\\"Images copied successfully.\\\")\";\n                var nbb_formatted_code = \"import os\\nimport shutil\\n\\ndata_path = \\\"classifications\\\"\\n\\n# List of subfolders\\nsubfolders = [\\\"daisy\\\", \\\"rose\\\", \\\"dandelion\\\"]\\n\\nCLASS_SAMPLES = 30\\n\\n# Define the source and destination directories\\nsource_dir = \\\"./images/flowers\\\"\\ntrain_img_dir = f\\\"./data/{data_path}/train_imgs\\\"\\n\\ntrain_lst_dir = f\\\"./data/{data_path}/train_annots/train.lst\\\"\\n\\n# Check if the destination directory exists and contains files\\nif os.path.exists(train_img_dir) and any(os.scandir(train_img_dir)):\\n    print(\\\"Destination directory already exists and contains files. Skipping copy.\\\")\\nelse:\\n    # Create the destination directory if it doesn't exist\\n    os.makedirs(train_img_dir, exist_ok=True)\\n\\n    # Create a new train.lst file, and remove the old one if it exists\\n    if os.path.exists(train_lst_dir):\\n        os.remove(train_lst_dir)\\n    os.makedirs(os.path.dirname(train_lst_dir), exist_ok=True)\\n\\n    with open(train_lst_dir, \\\"w\\\") as train_lst_file:\\n        # Copy the first CLASS_SAMPLES number of images from each subfolder\\n        line_number = 1\\n        for subfolder in subfolders:\\n            subfolder_path = os.path.join(source_dir, subfolder)\\n            destination_subfolder_path = train_img_dir\\n            os.makedirs(destination_subfolder_path, exist_ok=True)\\n\\n            # Get the list of image files in the subfolder\\n            image_files = [f for f in os.listdir(subfolder_path) if f.endswith(\\\".jpg\\\")]\\n\\n            # Copy the first CLASS_SAMPLES number of images\\n            classification_number = subfolders.index(subfolder)\\n            for image_file in image_files[:CLASS_SAMPLES]:\\n                src_file = os.path.join(subfolder_path, image_file)\\n                dst_file = os.path.join(destination_subfolder_path, image_file)\\n                shutil.copy(src_file, dst_file)\\n                # Write to train.lst file\\n                train_lst_file.write(\\n                    f\\\"{line_number}\\\\t{classification_number}\\\\t{image_file}\\\\n\\\"\\n                )\\n                line_number += 1\\n    print(\\\"Images copied successfully.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_path = \"classifications\"\n",
    "\n",
    "# List of subfolders, Order matters, as we will use the index as the classification number\n",
    "subfolders = ['daisy', 'rose', 'dandelion']\n",
    "\n",
    "CLASS_SAMPLES = 30\n",
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = './images/flowers'\n",
    "train_img_dir = f'./data/{data_path}/train_imgs'\n",
    "\n",
    "train_lst_dir = f'./data/{data_path}/train_annots/train.lst'\n",
    "\n",
    "# Check if the destination directory exists and contains files\n",
    "if os.path.exists(train_img_dir) and any(os.scandir(train_img_dir)):\n",
    "  print(\"Destination directory already exists and contains files. Skipping copy.\")\n",
    "else:\n",
    "  # Create the destination directory if it doesn't exist\n",
    "  os.makedirs(train_img_dir, exist_ok=True)\n",
    "\n",
    "  # Create a new train.lst file, and remove the old one if it exists\n",
    "  if os.path.exists(train_lst_dir):\n",
    "    os.remove(train_lst_dir)\n",
    "  os.makedirs(os.path.dirname(train_lst_dir), exist_ok=True)\n",
    "\n",
    "  with open(train_lst_dir, 'w') as train_lst_file:\n",
    "    # Copy the first CLASS_SAMPLES number of images from each subfolder\n",
    "    line_number = 1\n",
    "    for subfolder in subfolders:\n",
    "      subfolder_path = os.path.join(source_dir, subfolder)\n",
    "      destination_subfolder_path = train_img_dir\n",
    "      os.makedirs(destination_subfolder_path, exist_ok=True)\n",
    "\n",
    "      # Get the list of image files in the subfolder\n",
    "      image_files = [f for f in os.listdir(subfolder_path) if f.endswith('.jpg')]\n",
    "\n",
    "      # Copy the first CLASS_SAMPLES number of images\n",
    "      classification_number = subfolders.index(subfolder)\n",
    "      for image_file in image_files[:CLASS_SAMPLES]:\n",
    "        src_file = os.path.join(subfolder_path, image_file)\n",
    "        dst_file = os.path.join(destination_subfolder_path, image_file)\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        # Write to train.lst file\n",
    "        train_lst_file.write(f\"{line_number}\\t{classification_number}\\t{image_file}\\n\")\n",
    "        line_number += 1\n",
    "  print(\"Images copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27271906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0\t2057816617_18448093d0_n.jpg\n",
      "2\t0\t15760153042_a2a90e9da5_m.jpg\n",
      "3\t0\t18711159980_11d3bd5042.jpg\n",
      "4\t0\t162362896_99c7d851c8_n.jpg\n",
      "5\t0\t25360380_1a881a5648.jpg\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"!head -n 5 ./data/{data_path}/train_annots/train.lst\";\n                var nbb_formatted_code = \"!head -n 5 ./data/{data_path}/train_annots/train.lst\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!head -n 5 ./data/{data_path}/train_annots/train.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82fbf4d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in ./data/classifications/train_imgs:\n",
      "90\n",
      "Last 5 files with oldest timestamp in ./data/classifications/train_imgs:\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  68927 Jan 12 17:54 3483303007_42e3f90da7.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  24879 Jan 12 17:54 5434901893_4550be3f84_m.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  11656 Jan 12 17:54 5904946193_bd1eb1f39d_n.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  13514 Jan 12 17:54 7188513571_c8527b123a_n.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  98776 Jan 12 17:54 8489463746_a9839bf7e4.jpg\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"!echo \\\"Number of files in {train_img_dir}:\\\"\\n!ls -1q {train_img_dir} | wc -l\\n\\n!echo \\\"Last 5 files with oldest timestamp in {train_img_dir}:\\\"\\n!ls -lt {train_img_dir} | tail -n +2 | tail -n 5\";\n                var nbb_formatted_code = \"!echo \\\"Number of files in {train_img_dir}:\\\"\\n!ls -1q {train_img_dir} | wc -l\\n\\n!echo \\\"Last 5 files with oldest timestamp in {train_img_dir}:\\\"\\n!ls -lt {train_img_dir} | tail -n +2 | tail -n 5\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!echo \"Number of files in {train_img_dir}:\"\n",
    "!ls -1q {train_img_dir} | wc -l\n",
    "\n",
    "!echo \"Last 5 files with oldest timestamp in {train_img_dir}:\"\n",
    "!ls -lt {train_img_dir} | tail -n +2 | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ffe66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation images copied successfully.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"valid_lst_dir = f'./data/{data_path}/valid_annots/valid.lst'\\nVALIDATION_SAMPLES = 15\\n\\n# Create a new valid.lst file, and remove the old one if it exists\\nif os.path.exists(valid_lst_dir):\\n  os.remove(valid_lst_dir)\\nos.makedirs(os.path.dirname(valid_lst_dir), exist_ok=True)\\n\\n# Line number for the valid.lst file will start from the last line number of the train.lst file\\nline_number = CLASS_SAMPLES * len(subfolders) + 1\\n\\nvalid_imgs_dir = f'./data/{data_path}/valid_imgs'\\n\\nwith open(valid_lst_dir, 'w') as valid_lst_file:\\n  for subfolder in subfolders:\\n    subfolder_path = os.path.join(source_dir, subfolder)\\n    destination_subfolder_path = valid_imgs_dir\\n    os.makedirs(destination_subfolder_path, exist_ok=True)\\n\\n    # Get the list of image files in the subfolder\\n    image_files = [f for f in os.listdir(subfolder_path) if f.endswith('.jpg')]\\n\\n    # Copy the first [20,30] images to the validation folder\\n    classification_number = subfolders.index(subfolder)\\n    for image_file in image_files[CLASS_SAMPLES:CLASS_SAMPLES+VALIDATION_SAMPLES]:\\n      src_file = os.path.join(subfolder_path, image_file)\\n      dst_file = os.path.join(destination_subfolder_path, image_file)\\n      shutil.copy(src_file, dst_file)\\n      # Write to valid.lst file\\n      valid_lst_file.write(f\\\"{line_number}\\\\t{classification_number}\\\\t{image_file}\\\\n\\\")\\n      line_number += 1\\nprint(\\\"Validation images copied successfully.\\\")\";\n                var nbb_formatted_code = \"valid_lst_dir = f\\\"./data/{data_path}/valid_annots/valid.lst\\\"\\nVALIDATION_SAMPLES = 15\\n\\n# Create a new valid.lst file, and remove the old one if it exists\\nif os.path.exists(valid_lst_dir):\\n    os.remove(valid_lst_dir)\\nos.makedirs(os.path.dirname(valid_lst_dir), exist_ok=True)\\n\\n# Line number for the valid.lst file will start from the last line number of the train.lst file\\nline_number = CLASS_SAMPLES * len(subfolders) + 1\\n\\nvalid_imgs_dir = f\\\"./data/{data_path}/valid_imgs\\\"\\n\\nwith open(valid_lst_dir, \\\"w\\\") as valid_lst_file:\\n    for subfolder in subfolders:\\n        subfolder_path = os.path.join(source_dir, subfolder)\\n        destination_subfolder_path = valid_imgs_dir\\n        os.makedirs(destination_subfolder_path, exist_ok=True)\\n\\n        # Get the list of image files in the subfolder\\n        image_files = [f for f in os.listdir(subfolder_path) if f.endswith(\\\".jpg\\\")]\\n\\n        # Copy the first [20,30] images to the validation folder\\n        classification_number = subfolders.index(subfolder)\\n        for image_file in image_files[\\n            CLASS_SAMPLES : CLASS_SAMPLES + VALIDATION_SAMPLES\\n        ]:\\n            src_file = os.path.join(subfolder_path, image_file)\\n            dst_file = os.path.join(destination_subfolder_path, image_file)\\n            shutil.copy(src_file, dst_file)\\n            # Write to valid.lst file\\n            valid_lst_file.write(\\n                f\\\"{line_number}\\\\t{classification_number}\\\\t{image_file}\\\\n\\\"\\n            )\\n            line_number += 1\\nprint(\\\"Validation images copied successfully.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_lst_dir = f'./data/{data_path}/valid_annots/valid.lst'\n",
    "VALIDATION_SAMPLES = 15\n",
    "\n",
    "# Create a new valid.lst file, and remove the old one if it exists\n",
    "if os.path.exists(valid_lst_dir):\n",
    "  os.remove(valid_lst_dir)\n",
    "os.makedirs(os.path.dirname(valid_lst_dir), exist_ok=True)\n",
    "\n",
    "# Line number for the valid.lst file will start from the last line number of the train.lst file\n",
    "line_number = CLASS_SAMPLES * len(subfolders) + 1\n",
    "\n",
    "valid_imgs_dir = f'./data/{data_path}/valid_imgs'\n",
    "\n",
    "with open(valid_lst_dir, 'w') as valid_lst_file:\n",
    "  for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(source_dir, subfolder)\n",
    "    destination_subfolder_path = valid_imgs_dir\n",
    "    os.makedirs(destination_subfolder_path, exist_ok=True)\n",
    "\n",
    "    # Get the list of image files in the subfolder\n",
    "    image_files = [f for f in os.listdir(subfolder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Copy the first [20,30] images to the validation folder\n",
    "    classification_number = subfolders.index(subfolder)\n",
    "    for image_file in image_files[CLASS_SAMPLES:CLASS_SAMPLES+VALIDATION_SAMPLES]:\n",
    "      src_file = os.path.join(subfolder_path, image_file)\n",
    "      dst_file = os.path.join(destination_subfolder_path, image_file)\n",
    "      shutil.copy(src_file, dst_file)\n",
    "      # Write to valid.lst file\n",
    "      valid_lst_file.write(f\"{line_number}\\t{classification_number}\\t{image_file}\\n\")\n",
    "      line_number += 1\n",
    "print(\"Validation images copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277e9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\t0\t3456403987_5bd5fa6ece_n.jpg\n",
      "92\t0\t5110107234_12ddc0206b_m.jpg\n",
      "93\t0\t7669550908_bc5a11276f_n.jpg\n",
      "94\t0\t2454280135_ac3aa75cdc_n.jpg\n",
      "95\t0\t34510103621_250ee7ae64_n.jpg\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"!head -n 5 {valid_lst_dir}\";\n                var nbb_formatted_code = \"!head -n 5 {valid_lst_dir}\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!head -n 5 {valid_lst_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfcf968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in ./data/classifications/valid_imgs:\n",
      "45\n",
      "Last 5 files with oldest timestamp in ./data/classifications/valid_imgs:\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  47127 Jan 12 17:54 6480809573_76a0074b69_n.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  17506 Jan 12 17:54 6884975451_c74f445d69_m.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  19686 Jan 12 17:54 7669550908_bc5a11276f_n.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  24953 Jan 12 17:54 8619103877_d8c82c5f34_n.jpg\n",
      "-rw-rw-r-- 1 thangtran3112 thangtran3112  57158 Jan 12 17:54 8983779970_9d3a6a3bf2_n.jpg\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"!echo \\\"Number of files in {valid_imgs_dir}:\\\"\\n!ls -1q {valid_imgs_dir} | wc -l\\n\\n!echo \\\"Last 5 files with oldest timestamp in {valid_imgs_dir}:\\\"\\n!ls -lt {valid_imgs_dir} | tail -n +2 | tail -n 5\";\n                var nbb_formatted_code = \"!echo \\\"Number of files in {valid_imgs_dir}:\\\"\\n!ls -1q {valid_imgs_dir} | wc -l\\n\\n!echo \\\"Last 5 files with oldest timestamp in {valid_imgs_dir}:\\\"\\n!ls -lt {valid_imgs_dir} | tail -n +2 | tail -n 5\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!echo \"Number of files in {valid_imgs_dir}:\"\n",
    "!ls -1q {valid_imgs_dir} | wc -l\n",
    "\n",
    "!echo \"Last 5 files with oldest timestamp in {valid_imgs_dir}:\"\n",
    "!ls -lt {valid_imgs_dir} | tail -n +2 | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547f0395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"\\nimport json\\nimport logging\\nfrom datetime import datetime\";\n                var nbb_formatted_code = \"import json\\nimport logging\\nfrom datetime import datetime\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8863cf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_S3_BUCKET: sgmkr-images-training\n",
      "REGION: us-west-2\n",
      "SAGE_MAKER_LOCAL_ROLE: arn:aws:iam::654654352356:role/service-role/AmazonSageMaker-ExecutionRole-20250111T085887\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"import os\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n# Define the bucket name and region\\nIMAGE_S3_BUCKET = os.getenv(\\\"IMAGE_S3_BUCKET\\\")\\nREGION = os.getenv(\\\"REGION\\\")\\nSAGE_MAKER_LOCAL_ROLE = os.getenv(\\\"SAGE_MAKER_LOCAL_ROLE\\\")\\nprint(f\\\"IMAGE_S3_BUCKET: {IMAGE_S3_BUCKET}\\\")\\nprint(f\\\"REGION: {REGION}\\\")\\nprint(f\\\"SAGE_MAKER_LOCAL_ROLE: {SAGE_MAKER_LOCAL_ROLE}\\\")\";\n                var nbb_formatted_code = \"import os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n# Define the bucket name and region\\nIMAGE_S3_BUCKET = os.getenv(\\\"IMAGE_S3_BUCKET\\\")\\nREGION = os.getenv(\\\"REGION\\\")\\nSAGE_MAKER_LOCAL_ROLE = os.getenv(\\\"SAGE_MAKER_LOCAL_ROLE\\\")\\nprint(f\\\"IMAGE_S3_BUCKET: {IMAGE_S3_BUCKET}\\\")\\nprint(f\\\"REGION: {REGION}\\\")\\nprint(f\\\"SAGE_MAKER_LOCAL_ROLE: {SAGE_MAKER_LOCAL_ROLE}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Define the bucket name and region\n",
    "IMAGE_S3_BUCKET = os.getenv(\"IMAGE_S3_BUCKET\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "SAGE_MAKER_LOCAL_ROLE = os.getenv(\"SAGE_MAKER_LOCAL_ROLE\")\n",
    "print(f\"IMAGE_S3_BUCKET: {IMAGE_S3_BUCKET}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"SAGE_MAKER_LOCAL_ROLE: {SAGE_MAKER_LOCAL_ROLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94485bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'sgmkr-images-training' already exists.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"# Create the S3 bucket for Images, if it not exists\\nimport boto3\\nfrom botocore.exceptions import ClientError\\n\\n# Initialize S3 client with the specified region\\ns3_client = boto3.client('s3', region_name=REGION)\\n\\n# Function to check if bucket exists\\ndef bucket_exists(bucket_name):\\n    try:\\n        s3_client.head_bucket(Bucket=bucket_name)\\n        return True\\n    except ClientError:\\n        return False\\n\\n# Function to create bucket if it doesn't exist\\ndef create_bucket(bucket_name, region):\\n    try:\\n        if not bucket_exists(bucket_name):\\n            s3_client.create_bucket(\\n                Bucket=bucket_name,\\n                CreateBucketConfiguration={'LocationConstraint': region}\\n            )\\n            print(f\\\"Bucket '{bucket_name}' created successfully in region '{region}'.\\\")\\n        else:\\n            print(f\\\"Bucket '{bucket_name}' already exists.\\\")\\n    except ClientError as e:\\n        print(f\\\"Error creating bucket: {e}\\\")\\n\\n# Create the bucket\\ncreate_bucket(IMAGE_S3_BUCKET, REGION)\";\n                var nbb_formatted_code = \"# Create the S3 bucket for Images, if it not exists\\nimport boto3\\nfrom botocore.exceptions import ClientError\\n\\n# Initialize S3 client with the specified region\\ns3_client = boto3.client(\\\"s3\\\", region_name=REGION)\\n\\n\\n# Function to check if bucket exists\\ndef bucket_exists(bucket_name):\\n    try:\\n        s3_client.head_bucket(Bucket=bucket_name)\\n        return True\\n    except ClientError:\\n        return False\\n\\n\\n# Function to create bucket if it doesn't exist\\ndef create_bucket(bucket_name, region):\\n    try:\\n        if not bucket_exists(bucket_name):\\n            s3_client.create_bucket(\\n                Bucket=bucket_name,\\n                CreateBucketConfiguration={\\\"LocationConstraint\\\": region},\\n            )\\n            print(f\\\"Bucket '{bucket_name}' created successfully in region '{region}'.\\\")\\n        else:\\n            print(f\\\"Bucket '{bucket_name}' already exists.\\\")\\n    except ClientError as e:\\n        print(f\\\"Error creating bucket: {e}\\\")\\n\\n\\n# Create the bucket\\ncreate_bucket(IMAGE_S3_BUCKET, REGION)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the S3 bucket for Images, if it not exists\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize S3 client with the specified region\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "\n",
    "# Function to check if bucket exists\n",
    "def bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "# Function to create bucket if it doesn't exist\n",
    "def create_bucket(bucket_name, region):\n",
    "    try:\n",
    "        if not bucket_exists(bucket_name):\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "            print(f\"Bucket '{bucket_name}' created successfully in region '{region}'.\")\n",
    "        else:\n",
    "            print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "\n",
    "# Create the bucket\n",
    "create_bucket(IMAGE_S3_BUCKET, REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9326763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 17:54:35] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 17:54:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=213039;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=683668;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/thangtran3112/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"import boto3\\nimport sagemaker\\nfrom sagemaker import get_execution_role\\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\";\n                var nbb_formatted_code = \"import boto3\\nimport sagemaker\\nfrom sagemaker import get_execution_role\\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78e5f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"boto3.set_stream_logger(name=\\\"botocore.credentials\\\", level=logging.WARNING)\";\n                var nbb_formatted_code = \"boto3.set_stream_logger(name=\\\"botocore.credentials\\\", level=logging.WARNING)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boto3.set_stream_logger(name=\"botocore.credentials\", level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d68778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"sess = sagemaker.Session()\\nregion = sess.boto_region_name\\nprint(region)\";\n                var nbb_formatted_code = \"sess = sagemaker.Session()\\nregion = sess.boto_region_name\\nprint(region)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bce8711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in SageMaker Studio. Using custom role for local computer\n",
      "arn:aws:iam::654654352356:role/service-role/AmazonSageMaker-ExecutionRole-20250111T085887\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"if \\\"SM_CURRENT_HOST\\\" in os.environ:\\n  print(\\\"Running in SageMaker Studio\\\")\\n  # only inside Sagemaker notebook Studio\\n  role_arn = sagemaker.get_execution_role()\\nelse:\\n  print(\\\"Not running in SageMaker Studio. Using custom role for local computer\\\")\\n  # in local computer, we will get it from environment variable\\n  role_arn = SAGE_MAKER_LOCAL_ROLE\\n\\nprint(role_arn)\";\n                var nbb_formatted_code = \"if \\\"SM_CURRENT_HOST\\\" in os.environ:\\n    print(\\\"Running in SageMaker Studio\\\")\\n    # only inside Sagemaker notebook Studio\\n    role_arn = sagemaker.get_execution_role()\\nelse:\\n    print(\\\"Not running in SageMaker Studio. Using custom role for local computer\\\")\\n    # in local computer, we will get it from environment variable\\n    role_arn = SAGE_MAKER_LOCAL_ROLE\\n\\nprint(role_arn)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"SM_CURRENT_HOST\" in os.environ:\n",
    "  print(\"Running in SageMaker Studio\")\n",
    "  # only inside Sagemaker notebook Studio\n",
    "  role_arn = sagemaker.get_execution_role()\n",
    "else:\n",
    "  print(\"Not running in SageMaker Studio. Using custom role for local computer\")\n",
    "  # in local computer, we will get it from environment variable\n",
    "  role_arn = SAGE_MAKER_LOCAL_ROLE\n",
    "\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02ad82",
   "metadata": {},
   "source": [
    "## Uploading train and validation data to s3\n",
    "* Uploading the `train_imgs` images into the corresponding S3 bucket\n",
    "* Uploading the `valid_imgs` images into the corresponding S3 bucket\n",
    "* Upload `data/{data_path}/train_annots/train.lst` to the corresponding key in s3\n",
    "* Upload `data/{data_path}/valid_annots/valid.lst` to the corresponding key in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeaff2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 prefix at train_imgs already exists. Skipping upload.\n",
      "Deleted all objects under prefix classifications/train_imgs in bucket sgmkr-images-training.\n",
      "Train images uploaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"# Upload the images to the S3 bucket, if the prefix doesn't exist\\ns3_train_imgs_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_imgs\\\")\\n\\n# Function to check if S3 prefix exists\\ndef prefix_exists(bucket_name, prefix):\\n  result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n  return 'Contents' in result\\n\\n# Upload files to S3 if the prefix doesn't exist\\nif not prefix_exists(IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs\\\"):\\n  for root, dirs, files in os.walk(train_img_dir):\\n    for file in files:\\n      if file.endswith(\\\".jpg\\\"):\\n        file_path = os.path.join(root, file)\\n        s3_key = os.path.relpath(file_path, start=train_img_dir)\\n        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs/{s3_key}\\\")\\n  print(\\\"Train images uploaded successfully.\\\")\\nelse:\\n  print(\\\"S3 prefix at train_imgs already exists. Skipping upload.\\\")\\n  # Delete everything under s3_train_imgs_path prefix\\n  def delete_s3_prefix(bucket_name, prefix):\\n    objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n    if 'Contents' in objects_to_delete:\\n      delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]}\\n      s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\\n      print(f\\\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\\\")\\n\\n  delete_s3_prefix(IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs\\\")\\n\\n  # Upload files to S3\\n  for root, dirs, files in os.walk(train_img_dir):\\n    for file in files:\\n      if file.endswith(\\\".jpg\\\"):\\n        file_path = os.path.join(root, file)\\n        s3_key = os.path.relpath(file_path, start=train_img_dir)\\n        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs/{s3_key}\\\")\\n  print(\\\"Train images uploaded successfully.\\\")\";\n                var nbb_formatted_code = \"# Upload the images to the S3 bucket, if the prefix doesn't exist\\ns3_train_imgs_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_imgs\\\")\\n\\n\\n# Function to check if S3 prefix exists\\ndef prefix_exists(bucket_name, prefix):\\n    result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n    return \\\"Contents\\\" in result\\n\\n\\n# Upload files to S3 if the prefix doesn't exist\\nif not prefix_exists(IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs\\\"):\\n    for root, dirs, files in os.walk(train_img_dir):\\n        for file in files:\\n            if file.endswith(\\\".jpg\\\"):\\n                file_path = os.path.join(root, file)\\n                s3_key = os.path.relpath(file_path, start=train_img_dir)\\n                s3_client.upload_file(\\n                    file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs/{s3_key}\\\"\\n                )\\n    print(\\\"Train images uploaded successfully.\\\")\\nelse:\\n    print(\\\"S3 prefix at train_imgs already exists. Skipping upload.\\\")\\n\\n    # Delete everything under s3_train_imgs_path prefix\\n    def delete_s3_prefix(bucket_name, prefix):\\n        objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n        if \\\"Contents\\\" in objects_to_delete:\\n            delete_keys = {\\n                \\\"Objects\\\": [\\n                    {\\\"Key\\\": obj[\\\"Key\\\"]} for obj in objects_to_delete[\\\"Contents\\\"]\\n                ]\\n            }\\n            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\\n            print(f\\\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\\\")\\n\\n    delete_s3_prefix(IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs\\\")\\n\\n    # Upload files to S3\\n    for root, dirs, files in os.walk(train_img_dir):\\n        for file in files:\\n            if file.endswith(\\\".jpg\\\"):\\n                file_path = os.path.join(root, file)\\n                s3_key = os.path.relpath(file_path, start=train_img_dir)\\n                s3_client.upload_file(\\n                    file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/train_imgs/{s3_key}\\\"\\n                )\\n    print(\\\"Train images uploaded successfully.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload the images to the S3 bucket, if the prefix doesn't exist\n",
    "s3_train_imgs_path = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"train_imgs\")\n",
    "\n",
    "# Function to check if S3 prefix exists\n",
    "def prefix_exists(bucket_name, prefix):\n",
    "  result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "  return 'Contents' in result\n",
    "\n",
    "# Upload files to S3 if the prefix doesn't exist\n",
    "if not prefix_exists(IMAGE_S3_BUCKET, f\"{data_path}/train_imgs\"):\n",
    "  for root, dirs, files in os.walk(train_img_dir):\n",
    "    for file in files:\n",
    "      if file.endswith(\".jpg\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        s3_key = os.path.relpath(file_path, start=train_img_dir)\n",
    "        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\"{data_path}/train_imgs/{s3_key}\")\n",
    "  print(\"Train images uploaded successfully.\")\n",
    "else:\n",
    "  print(\"S3 prefix at train_imgs already exists. Skipping upload.\")\n",
    "  # Delete everything under s3_train_imgs_path prefix\n",
    "  def delete_s3_prefix(bucket_name, prefix):\n",
    "    objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' in objects_to_delete:\n",
    "      delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]}\n",
    "      s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "      print(f\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\")\n",
    "\n",
    "  delete_s3_prefix(IMAGE_S3_BUCKET, f\"{data_path}/train_imgs\")\n",
    "\n",
    "  # Upload files to S3\n",
    "  for root, dirs, files in os.walk(train_img_dir):\n",
    "    for file in files:\n",
    "      if file.endswith(\".jpg\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        s3_key = os.path.relpath(file_path, start=train_img_dir)\n",
    "        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\"{data_path}/train_imgs/{s3_key}\")\n",
    "  print(\"Train images uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71b9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-12 17:54:50      26797 classifications/train_imgs/100080576_f52e8ee070_n.jpg\n",
      "2025-01-12 17:54:50      12128 classifications/train_imgs/12094442595_297494dba4_m.jpg\n",
      "2025-01-12 17:54:49      36234 classifications/train_imgs/12338444334_72fcc2fc58_m.jpg\n",
      "2025-01-12 17:54:42      14758 classifications/train_imgs/14060367700_fe87e99b6a_m.jpg\n",
      "2025-01-12 17:54:43      41143 classifications/train_imgs/14171812905_8b81d50eb9_n.jpg\n",
      "2025-01-12 17:54:45      91426 classifications/train_imgs/14396023703_11c5dd35a9.jpg\n",
      "2025-01-12 17:54:46     129826 classifications/train_imgs/1445228333_59a07e0801.jpg\n",
      "2025-01-12 17:54:44      88145 classifications/train_imgs/14886860069_b84665a073.jpg\n",
      "2025-01-12 17:54:51      26816 classifications/train_imgs/15029936576_8d6f96c72c_n.jpg\n",
      "2025-01-12 17:54:42      24577 classifications/train_imgs/15760153042_a2a90e9da5_m.jpg\n",
      "\n",
      "[Errno 32] Broken pipe\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 19;\n                var nbb_unformatted_code = \"!aws s3 ls {s3_train_imgs_path} --recursive | head -n 10\";\n                var nbb_formatted_code = \"!aws s3 ls {s3_train_imgs_path} --recursive | head -n 10\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!aws s3 ls {s3_train_imgs_path} --recursive | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d024024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 prefix at valid_imgs already exists. Skipping upload.\n",
      "Deleted all objects under prefix classifications/valid_imgs in bucket sgmkr-images-training.\n",
      "Valid images uploaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 20;\n                var nbb_unformatted_code = \"# Upload the images to the S3 bucket, if the prefix doesn't exist\\ns3_valid_imgs_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_imgs\\\")\\n\\n# Upload files to S3 if the prefix doesn't exist\\nif not prefix_exists(IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs\\\"):\\n  for root, dirs, files in os.walk(valid_imgs_dir):\\n    for file in files:\\n      if file.endswith(\\\".jpg\\\"):\\n        file_path = os.path.join(root, file)\\n        s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\\n        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs/{s3_key}\\\")\\n  print(\\\"Valid images uploaded successfully.\\\")\\nelse:\\n  print(\\\"S3 prefix at valid_imgs already exists. Skipping upload.\\\")\\n  # Delete everything under s3_valid_imgs_path prefix\\n  def delete_s3_prefix(bucket_name, prefix):\\n    objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n    if 'Contents' in objects_to_delete:\\n      delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]}\\n      s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\\n      print(f\\\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\\\")\\n\\n  delete_s3_prefix(IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs\\\")\\n\\n  # Upload files to S3\\n  for root, dirs, files in os.walk(valid_imgs_dir):\\n    for file in files:\\n      if file.endswith(\\\".jpg\\\"):\\n        file_path = os.path.join(root, file)\\n        s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\\n        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs/{s3_key}\\\")\\n  print(\\\"Valid images uploaded successfully.\\\")\";\n                var nbb_formatted_code = \"# Upload the images to the S3 bucket, if the prefix doesn't exist\\ns3_valid_imgs_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_imgs\\\")\\n\\n# Upload files to S3 if the prefix doesn't exist\\nif not prefix_exists(IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs\\\"):\\n    for root, dirs, files in os.walk(valid_imgs_dir):\\n        for file in files:\\n            if file.endswith(\\\".jpg\\\"):\\n                file_path = os.path.join(root, file)\\n                s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\\n                s3_client.upload_file(\\n                    file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs/{s3_key}\\\"\\n                )\\n    print(\\\"Valid images uploaded successfully.\\\")\\nelse:\\n    print(\\\"S3 prefix at valid_imgs already exists. Skipping upload.\\\")\\n\\n    # Delete everything under s3_valid_imgs_path prefix\\n    def delete_s3_prefix(bucket_name, prefix):\\n        objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\\n        if \\\"Contents\\\" in objects_to_delete:\\n            delete_keys = {\\n                \\\"Objects\\\": [\\n                    {\\\"Key\\\": obj[\\\"Key\\\"]} for obj in objects_to_delete[\\\"Contents\\\"]\\n                ]\\n            }\\n            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\\n            print(f\\\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\\\")\\n\\n    delete_s3_prefix(IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs\\\")\\n\\n    # Upload files to S3\\n    for root, dirs, files in os.walk(valid_imgs_dir):\\n        for file in files:\\n            if file.endswith(\\\".jpg\\\"):\\n                file_path = os.path.join(root, file)\\n                s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\\n                s3_client.upload_file(\\n                    file_path, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_imgs/{s3_key}\\\"\\n                )\\n    print(\\\"Valid images uploaded successfully.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload the images to the S3 bucket, if the prefix doesn't exist\n",
    "s3_valid_imgs_path = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"valid_imgs\")\n",
    "\n",
    "# Upload files to S3 if the prefix doesn't exist\n",
    "if not prefix_exists(IMAGE_S3_BUCKET, f\"{data_path}/valid_imgs\"):\n",
    "  for root, dirs, files in os.walk(valid_imgs_dir):\n",
    "    for file in files:\n",
    "      if file.endswith(\".jpg\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\n",
    "        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\"{data_path}/valid_imgs/{s3_key}\")\n",
    "  print(\"Valid images uploaded successfully.\")\n",
    "else:\n",
    "  print(\"S3 prefix at valid_imgs already exists. Skipping upload.\")\n",
    "  # Delete everything under s3_valid_imgs_path prefix\n",
    "  def delete_s3_prefix(bucket_name, prefix):\n",
    "    objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' in objects_to_delete:\n",
    "      delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]}\n",
    "      s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "      print(f\"Deleted all objects under prefix {prefix} in bucket {bucket_name}.\")\n",
    "\n",
    "  delete_s3_prefix(IMAGE_S3_BUCKET, f\"{data_path}/valid_imgs\")\n",
    "\n",
    "  # Upload files to S3\n",
    "  for root, dirs, files in os.walk(valid_imgs_dir):\n",
    "    for file in files:\n",
    "      if file.endswith(\".jpg\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        s3_key = os.path.relpath(file_path, start=valid_imgs_dir)\n",
    "        s3_client.upload_file(file_path, IMAGE_S3_BUCKET, f\"{data_path}/valid_imgs/{s3_key}\")\n",
    "  print(\"Valid images uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423fad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded ./data/classifications/train_annots/train.lst to s3://sgmkr-images-training/classifications/train_annots/train.lst\n",
      "Uploaded ./data/classifications/valid_annots/valid.lst to s3://sgmkr-images-training/classifications/valid_annots/valid.lst\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"# Define the S3 paths for the annotation files\\ns3_train_lst_path = f\\\"s3://{IMAGE_S3_BUCKET}/{data_path}/train_annots/train.lst\\\"\\ns3_valid_lst_path = f\\\"s3://{IMAGE_S3_BUCKET}/{data_path}/valid_annots/valid.lst\\\"\\n\\n# Function to upload file to S3, replacing if it exists\\ndef upload_file_to_s3(local_path, bucket, s3_path):\\n  s3_client.upload_file(local_path, bucket, s3_path)\\n  print(f\\\"Uploaded {local_path} to s3://{bucket}/{s3_path}\\\")\\n\\n# Upload the train.lst file to S3\\nupload_file_to_s3(train_lst_dir, IMAGE_S3_BUCKET, f\\\"{data_path}/train_annots/train.lst\\\")\\n\\n# Upload the valid.lst file to S3\\nupload_file_to_s3(valid_lst_dir, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_annots/valid.lst\\\")\";\n                var nbb_formatted_code = \"# Define the S3 paths for the annotation files\\ns3_train_lst_path = f\\\"s3://{IMAGE_S3_BUCKET}/{data_path}/train_annots/train.lst\\\"\\ns3_valid_lst_path = f\\\"s3://{IMAGE_S3_BUCKET}/{data_path}/valid_annots/valid.lst\\\"\\n\\n\\n# Function to upload file to S3, replacing if it exists\\ndef upload_file_to_s3(local_path, bucket, s3_path):\\n    s3_client.upload_file(local_path, bucket, s3_path)\\n    print(f\\\"Uploaded {local_path} to s3://{bucket}/{s3_path}\\\")\\n\\n\\n# Upload the train.lst file to S3\\nupload_file_to_s3(train_lst_dir, IMAGE_S3_BUCKET, f\\\"{data_path}/train_annots/train.lst\\\")\\n\\n# Upload the valid.lst file to S3\\nupload_file_to_s3(valid_lst_dir, IMAGE_S3_BUCKET, f\\\"{data_path}/valid_annots/valid.lst\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the S3 paths for the annotation files\n",
    "s3_train_lst_path = f\"s3://{IMAGE_S3_BUCKET}/{data_path}/train_annots/train.lst\"\n",
    "s3_valid_lst_path = f\"s3://{IMAGE_S3_BUCKET}/{data_path}/valid_annots/valid.lst\"\n",
    "\n",
    "# Function to upload file to S3, replacing if it exists\n",
    "def upload_file_to_s3(local_path, bucket, s3_path):\n",
    "  s3_client.upload_file(local_path, bucket, s3_path)\n",
    "  print(f\"Uploaded {local_path} to s3://{bucket}/{s3_path}\")\n",
    "\n",
    "# Upload the train.lst file to S3\n",
    "upload_file_to_s3(train_lst_dir, IMAGE_S3_BUCKET, f\"{data_path}/train_annots/train.lst\")\n",
    "\n",
    "# Upload the valid.lst file to S3\n",
    "upload_file_to_s3(valid_lst_dir, IMAGE_S3_BUCKET, f\"{data_path}/valid_annots/valid.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4620fe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 17:56:26] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/image_uris.py#528\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">528</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 17:56:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=961089;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324973;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/image_uris.py#528\u001b\\\u001b[2m528\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:1\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"# This is inbuilt image classification algorithm from AWS Sagemaker\\n# For this in-built algorithm, the training instance can also be used for inference\\ntrain_image_uri = sagemaker.image_uris.retrieve(\\n    framework=\\\"image-classification\\\",\\n    region=region,\\n    image_scope=\\\"training\\\",\\n    version=\\\"latest\\\",\\n)\\nprint(train_image_uri)\";\n                var nbb_formatted_code = \"# This is inbuilt image classification algorithm from AWS Sagemaker\\n# For this in-built algorithm, the training instance can also be used for inference\\ntrain_image_uri = sagemaker.image_uris.retrieve(\\n    framework=\\\"image-classification\\\",\\n    region=region,\\n    image_scope=\\\"training\\\",\\n    version=\\\"latest\\\",\\n)\\nprint(train_image_uri)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is inbuilt image classification algorithm from AWS Sagemaker\n",
    "# For this in-built algorithm, the training instance can also be used for inference\n",
    "train_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"image-classification\",\n",
    "    region=region,\n",
    "    image_scope=\"training\",\n",
    "    version=\"latest\",\n",
    ")\n",
    "print(train_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975382fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sgmkr-images-training/classifications/model_output'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"s3_output_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"model_output\\\")\\ns3_output_path\";\n                var nbb_formatted_code = \"s3_output_path = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"model_output\\\")\\ns3_output_path\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_output_path = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"model_output\")\n",
    "s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a54e9c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 24;\n                var nbb_unformatted_code = \"nclasses = 3\\nnimgs_train = nclasses * CLASS_SAMPLES\\nnepochs = 15\\nmini_batch_size = 8\\n\\ntrain_instance_type = \\\"ml.g4dn.xlarge\\\"\\njob_name_prefix = \\\"flowers-clf-ib-resent-\\\"\\nclf_estimator = sagemaker.estimator.Estimator(\\n    image_uri=train_image_uri,\\n    role=role_arn,\\n    instance_count=1, # Some models can be trained in parallel on multiple instances. But this model is not one of them.\\n    instance_type=train_instance_type,\\n    volume_size=50,\\n    max_run=360000,\\n    input_mode=\\\"File\\\",\\n    output_path=s3_output_path,\\n    sagemaker_session=sess,\\n)\";\n                var nbb_formatted_code = \"nclasses = 3\\nnimgs_train = nclasses * CLASS_SAMPLES\\nnepochs = 15\\nmini_batch_size = 8\\n\\ntrain_instance_type = \\\"ml.g4dn.xlarge\\\"\\njob_name_prefix = \\\"flowers-clf-ib-resent-\\\"\\nclf_estimator = sagemaker.estimator.Estimator(\\n    image_uri=train_image_uri,\\n    role=role_arn,\\n    instance_count=1,  # Some models can be trained in parallel on multiple instances. But this model is not one of them.\\n    instance_type=train_instance_type,\\n    volume_size=50,\\n    max_run=360000,\\n    input_mode=\\\"File\\\",\\n    output_path=s3_output_path,\\n    sagemaker_session=sess,\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nclasses = 3\n",
    "nimgs_train = nclasses * CLASS_SAMPLES\n",
    "nepochs = 15\n",
    "mini_batch_size = 8\n",
    "\n",
    "train_instance_type = \"ml.g4dn.xlarge\"\n",
    "job_name_prefix = \"flowers-clf-ib-resent-\"\n",
    "clf_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=train_image_uri,\n",
    "    role=role_arn,\n",
    "    instance_count=1, # Some models can be trained in parallel on multiple instances. But this model is not one of them.\n",
    "    instance_type=train_instance_type,\n",
    "    volume_size=50,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060494a1",
   "metadata": {},
   "source": [
    "## In-built image-classification models from MX AWS Sagemaker\n",
    "* In this particular notebook, we will train using [MX Image Classification Container](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html)\n",
    "* [Notebook documentation](https://github.com/aws-samples/amazon-sagemaker-pipelines-mxnet-image-classification/blob/main/image-classification-sagemaker-pipelines.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e61bdb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 26;\n                var nbb_unformatted_code = \"# https://github.com/aws-samples/amazon-sagemaker-pipelines-mxnet-image-classification/blob/main/image-classification-sagemaker-pipelines.ipynb\\n# https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\\nclf_estimator.set_hyperparameters(\\n    num_classes=nclasses,  # 3 classes as daiy, rose, dandelion\\n    epochs=nepochs,  # update this\\n    num_training_samples=nimgs_train,  # number of training images under train_imgs\\n    mini_batch_size=mini_batch_size,  # update this\\n    num_layers=18, # Only 17 layers will be retrained, other layers will be freezed. Last activation layer is classification layer\\n    use_pretrained_model=1,\\n    image_shape=\\\"3,224,224\\\",\\n    resize=256,\\n    learning_rate=0.001,\\n    use_weighted_loss=1,\\n    augmentation_type=\\\"crop_color_transform\\\",\\n    precision_dtype=\\\"float32\\\",\\n    multi_label=0, # our classification is single label, only 1 digit [0,1,2]\\n)\";\n                var nbb_formatted_code = \"# https://github.com/aws-samples/amazon-sagemaker-pipelines-mxnet-image-classification/blob/main/image-classification-sagemaker-pipelines.ipynb\\n# https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\\nclf_estimator.set_hyperparameters(\\n    num_classes=nclasses,  # 3 classes as daiy, rose, dandelion\\n    epochs=nepochs,  # update this\\n    num_training_samples=nimgs_train,  # number of training images under train_imgs\\n    mini_batch_size=mini_batch_size,  # update this\\n    num_layers=18,  # Only 17 layers will be retrained, other layers will be freezed. Last activation layer is classification layer\\n    use_pretrained_model=1,\\n    image_shape=\\\"3,224,224\\\",\\n    resize=256,\\n    learning_rate=0.001,\\n    use_weighted_loss=1,\\n    augmentation_type=\\\"crop_color_transform\\\",\\n    precision_dtype=\\\"float32\\\",\\n    multi_label=0,  # our classification is single label, only 1 digit [0,1,2]\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_estimator.set_hyperparameters(\n",
    "    num_classes=nclasses,  # 3 classes as daiy, rose, dandelion\n",
    "    epochs=nepochs,  # update this\n",
    "    num_training_samples=nimgs_train,  # number of training images under train_imgs\n",
    "    mini_batch_size=mini_batch_size,  # update this\n",
    "    num_layers=18, # Only 17 layers will be retrained, other layers will be freezed. Last activation layer is classification layer\n",
    "    use_pretrained_model=1,\n",
    "    image_shape=\"3,224,224\",\n",
    "    resize=256,\n",
    "    learning_rate=0.001, # use a larger learning rate for quicker convergence, but risk going over the optimal point\n",
    "    use_weighted_loss=1,\n",
    "    augmentation_type=\"crop_color_transform\",\n",
    "    precision_dtype=\"float32\",\n",
    "    multi_label=0, # our classification is single label, only 1 digit [0,1,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "104f505d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 27;\n                var nbb_unformatted_code = \"s3_train_imgs = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_imgs\\\")\\ns3_valid_imgs = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_imgs\\\")\\ns3_train_annot = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_annots\\\")\\ns3_valid_annot = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_annots\\\")\\n\\ntrain_imgs = sagemaker.inputs.TrainingInput(\\n    s3_train_imgs,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\nvalid_imgs = sagemaker.inputs.TrainingInput(\\n    s3_valid_imgs,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\ntrain_annot = sagemaker.inputs.TrainingInput(\\n    s3_train_annot,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\nvalid_annot = sagemaker.inputs.TrainingInput(\\n    s3_valid_annot,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\n\\ndata_channels = {\\n    \\\"train\\\": train_imgs,\\n    \\\"validation\\\": valid_imgs,\\n    \\\"train_lst\\\": train_annot,\\n    \\\"validation_lst\\\": valid_annot,\\n}\";\n                var nbb_formatted_code = \"s3_train_imgs = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_imgs\\\")\\ns3_valid_imgs = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_imgs\\\")\\ns3_train_annot = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"train_annots\\\")\\ns3_valid_annot = \\\"s3://{}/{}/{}\\\".format(IMAGE_S3_BUCKET, data_path, \\\"valid_annots\\\")\\n\\ntrain_imgs = sagemaker.inputs.TrainingInput(\\n    s3_train_imgs,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\nvalid_imgs = sagemaker.inputs.TrainingInput(\\n    s3_valid_imgs,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\ntrain_annot = sagemaker.inputs.TrainingInput(\\n    s3_train_annot,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\nvalid_annot = sagemaker.inputs.TrainingInput(\\n    s3_valid_annot,\\n    distribution=\\\"FullyReplicated\\\",\\n    content_type=\\\"application/jpeg\\\",\\n    s3_data_type=\\\"S3Prefix\\\",\\n)\\n\\ndata_channels = {\\n    \\\"train\\\": train_imgs,\\n    \\\"validation\\\": valid_imgs,\\n    \\\"train_lst\\\": train_annot,\\n    \\\"validation_lst\\\": valid_annot,\\n}\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_train_imgs = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"train_imgs\")\n",
    "s3_valid_imgs = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"valid_imgs\")\n",
    "s3_train_annot = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"train_annots\")\n",
    "s3_valid_annot = \"s3://{}/{}/{}\".format(IMAGE_S3_BUCKET, data_path, \"valid_annots\")\n",
    "\n",
    "train_imgs = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_imgs,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/jpeg\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "valid_imgs = sagemaker.inputs.TrainingInput(\n",
    "    s3_valid_imgs,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/jpeg\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "train_annot = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_annot,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/jpeg\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "valid_annot = sagemaker.inputs.TrainingInput(\n",
    "    s3_valid_annot,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/jpeg\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": train_imgs,\n",
    "    \"validation\": valid_imgs,\n",
    "    \"train_lst\": train_annot,\n",
    "    \"validation_lst\": valid_annot,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4da69d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers-clf-ib-resent-2025-01-12-17-57-19\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 28;\n                var nbb_unformatted_code = \"timestamp = (\\n    str(datetime.now().replace(microsecond=0)).replace(\\\" \\\", \\\"-\\\").replace(\\\":\\\", \\\"-\\\")\\n)\\njob_name = job_name_prefix + timestamp\\nprint(job_name)\";\n                var nbb_formatted_code = \"timestamp = (\\n    str(datetime.now().replace(microsecond=0)).replace(\\\" \\\", \\\"-\\\").replace(\\\":\\\", \\\"-\\\")\\n)\\njob_name = job_name_prefix + timestamp\\nprint(job_name)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = (\n",
    "    str(datetime.now().replace(microsecond=0)).replace(\" \", \"-\").replace(\":\", \"-\")\n",
    ")\n",
    "job_name = job_name_prefix + timestamp\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea95acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 17:57:21] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#90\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 17:57:21]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=759972;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=451907;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=980034;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=396269;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m                              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 01:57:23 Starting - Starting the training job...\n",
      "2025-01-13 01:57:37 Starting - Preparing the instances for training...\n",
      "2025-01-13 01:58:27 Downloading - Downloading the training image.........\n",
      "2025-01-13 01:59:43 Training - Training image download completed. Training in progress.Docker entrypoint called with argument(s): train\n",
      "Running default environment configuration script\n",
      "Nvidia gpu devices, drivers and cuda toolkit versions (only available on hosts with GPU):\n",
      "Mon Jan 13 01:59:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   23C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Checking for nvidia driver and cuda compatibility.\n",
      "CUDA Compatibility driver provided.\n",
      "Proceeding with compatibility check between driver, cuda-toolkit and cuda-compat.\n",
      "Detected cuda-toolkit version: 11.1.\n",
      "Detected cuda-compat version: 455.32.00.\n",
      "Detected Nvidia driver version: 550.127.05.\n",
      "Nvidia driver compatible with cuda-toolkit. Disabling cuda-compat.\n",
      "/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\n",
      "[01/13/2025 01:59:53 INFO 140702620559168] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\n",
      "[01/13/2025 01:59:53 INFO 140702620559168] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'augmentation_type': 'crop_color_transform', 'epochs': '15', 'image_shape': '3,224,224', 'learning_rate': '0.001', 'mini_batch_size': '8', 'multi_label': '0', 'num_classes': '3', 'num_layers': '18', 'num_training_samples': '90', 'precision_dtype': 'float32', 'resize': '256', 'use_pretrained_model': '1', 'use_weighted_loss': '1'}\n",
      "[01/13/2025 01:59:53 INFO 140702620559168] Final configuration: {'use_pretrained_model': '1', 'num_layers': '18', 'epochs': '15', 'learning_rate': '0.001', 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': '8', 'image_shape': '3,224,224', 'precision_dtype': 'float32', 'augmentation_type': 'crop_color_transform', 'multi_label': '0', 'num_classes': '3', 'num_training_samples': '90', 'resize': '256', 'use_weighted_loss': '1'}\n",
      "[01/13/2025 01:59:53 WARNING 140702620559168] use_weighted_loss is only used for multi-label training. Ignoring the parameter.\n",
      "[01/13/2025 01:59:53 INFO 140702620559168] Searching for .lst files in /opt/ml/input/data/train_lst.\n",
      "[01/13/2025 01:59:53 INFO 140702620559168] Creating record files for train.lst\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Done creating record files...\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Searching for .lst files in /opt/ml/input/data/validation_lst.\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Creating record files for valid.lst\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Done creating record files...\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] use_pretrained_model: 1\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] multi_label: 0\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Using pretrained model for initializing weights and transfer learning.\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] ---- Parameters ----\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] num_layers: 18\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] data type: <class 'numpy.float32'>\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] epochs: 15\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] image resize size: 256\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] optimizer: sgd\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] momentum: 0.9\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] weight_decay: 0.0001\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] learning_rate: 0.001\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] num_training_samples: 90\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] mini_batch_size: 8\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] image_shape: 3,224,224\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] num_classes: 3\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] augmentation_type: crop_color_transform\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] kv_store: device\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] checkpoint_frequency not set, will store the best model\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] --------------------\n",
      "[01:59:54] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.441.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\n",
      "[01:59:54] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.441.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\n",
      "/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "[01/13/2025 01:59:54 INFO 140702620559168] Setting number of threads: 3\n",
      "[01:59:58] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.441.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\n",
      "[01/13/2025 01:59:59 INFO 140702620559168] Epoch[0] Train-accuracy=0.511364\n",
      "[01/13/2025 01:59:59 INFO 140702620559168] Epoch[0] Time cost=0.870\n",
      "[01/13/2025 01:59:59 INFO 140702620559168] Epoch[0] Validation-accuracy=0.750000\n",
      "[01/13/2025 01:59:59 INFO 140702620559168] Storing the best model with validation accuracy: 0.750000\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Epoch[1] Train-accuracy=0.795455\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Epoch[1] Time cost=0.499\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Epoch[1] Validation-accuracy=0.900000\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Storing the best model with validation accuracy: 0.900000\n",
      "[01/13/2025 02:00:00 INFO 140702620559168] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\n",
      "[01/13/2025 02:00:01 INFO 140702620559168] Epoch[2] Train-accuracy=0.806818\n",
      "[01/13/2025 02:00:01 INFO 140702620559168] Epoch[2] Time cost=0.446\n",
      "[01/13/2025 02:00:01 INFO 140702620559168] Epoch[2] Validation-accuracy=0.875000\n",
      "[01/13/2025 02:00:02 INFO 140702620559168] Epoch[3] Train-accuracy=0.863636\n",
      "[01/13/2025 02:00:02 INFO 140702620559168] Epoch[3] Time cost=0.715\n",
      "[01/13/2025 02:00:02 INFO 140702620559168] Epoch[3] Validation-accuracy=0.875000\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Epoch[4] Train-accuracy=0.931818\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Epoch[4] Time cost=0.427\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Epoch[4] Validation-accuracy=0.950000\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Storing the best model with validation accuracy: 0.950000\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Epoch[5] Train-accuracy=0.920455\n",
      "[01/13/2025 02:00:03 INFO 140702620559168] Epoch[5] Time cost=0.438\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Epoch[5] Validation-accuracy=0.854167\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Epoch[6] Train-accuracy=0.920455\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Epoch[6] Time cost=0.395\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Epoch[6] Validation-accuracy=0.975000\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Storing the best model with validation accuracy: 0.975000\n",
      "[01/13/2025 02:00:04 INFO 140702620559168] Saved checkpoint to \"/opt/ml/model/image-classification-0007.params\"\n",
      "[01/13/2025 02:00:05 INFO 140702620559168] Epoch[7] Train-accuracy=0.988636\n",
      "[01/13/2025 02:00:05 INFO 140702620559168] Epoch[7] Time cost=0.705\n",
      "[01/13/2025 02:00:05 INFO 140702620559168] Epoch[7] Validation-accuracy=0.895833\n",
      "[01/13/2025 02:00:06 INFO 140702620559168] Epoch[8] Train-accuracy=0.943182\n",
      "[01/13/2025 02:00:06 INFO 140702620559168] Epoch[8] Time cost=0.432\n",
      "[01/13/2025 02:00:06 INFO 140702620559168] Epoch[8] Validation-accuracy=0.895833\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[9] Train-accuracy=0.954545\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[9] Time cost=0.437\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[9] Validation-accuracy=0.950000\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[10] Train-accuracy=0.954545\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[10] Time cost=0.397\n",
      "[01/13/2025 02:00:07 INFO 140702620559168] Epoch[10] Validation-accuracy=0.937500\n",
      "[01/13/2025 02:00:08 INFO 140702620559168] Epoch[11] Train-accuracy=0.954545\n",
      "[01/13/2025 02:00:08 INFO 140702620559168] Epoch[11] Time cost=0.697\n",
      "[01/13/2025 02:00:08 INFO 140702620559168] Epoch[11] Validation-accuracy=0.916667\n",
      "[01/13/2025 02:00:09 INFO 140702620559168] Epoch[12] Train-accuracy=0.954545\n",
      "[01/13/2025 02:00:09 INFO 140702620559168] Epoch[12] Time cost=0.439\n",
      "[01/13/2025 02:00:09 INFO 140702620559168] Epoch[12] Validation-accuracy=0.975000\n",
      "[01/13/2025 02:00:10 INFO 140702620559168] Epoch[13] Train-accuracy=0.920455\n",
      "[01/13/2025 02:00:10 INFO 140702620559168] Epoch[13] Time cost=0.445\n",
      "[01/13/2025 02:00:10 INFO 140702620559168] Epoch[13] Validation-accuracy=0.916667\n",
      "[01/13/2025 02:00:10 INFO 140702620559168] Epoch[14] Train-accuracy=0.977273\n",
      "[01/13/2025 02:00:10 INFO 140702620559168] Epoch[14] Time cost=0.396\n",
      "[01/13/2025 02:00:11 INFO 140702620559168] Epoch[14] Validation-accuracy=0.950000\n",
      "\n",
      "2025-01-13 02:00:31 Uploading - Uploading generated training model\n",
      "2025-01-13 02:00:31 Completed - Training job completed\n",
      "Training seconds: 149\n",
      "Billable seconds: 149\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 29;\n                var nbb_unformatted_code = \"clf_estimator.fit(inputs=data_channels, logs=True, job_name=job_name)\";\n                var nbb_formatted_code = \"clf_estimator.fit(inputs=data_channels, logs=True, job_name=job_name)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_estimator.fit(inputs=data_channels, logs=True, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0890d84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"infer_instance_type = \\\"ml.t2.medium\\\"\\nmodel_name = job_name\\nendpoint_name = job_name\";\n                var nbb_formatted_code = \"infer_instance_type = \\\"ml.t2.medium\\\"\\nmodel_name = job_name\\nendpoint_name = job_name\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_instance_type = \"ml.t2.medium\"\n",
    "model_name = job_name\n",
    "endpoint_name = job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849905f",
   "metadata": {},
   "source": [
    "## Using existing estimator to deploy inference models\n",
    "* In real-world, we could also deploy the endpoint using S3 weighted artifact under {IMAGE_S3_BUCKET}/model-output to deploy Inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0968d19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 18:00:55] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>    <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 18:00:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m    \u001b]8;id=314335;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=714694;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 18:00:56] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#5889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5889</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 18:00:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=381658;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=67711;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#5889\u001b\\\u001b[2m5889\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m                              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>  <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4711\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4711</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m  \u001b]8;id=572295;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=590950;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4711\u001b\\\u001b[2m4711\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------!"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"clf_predictor = clf_estimator.deploy(\\n    initial_instance_count=1,\\n    instance_type=infer_instance_type,\\n    endpoint_name=endpoint_name,\\n    model_name=model_name,\\n)\";\n                var nbb_formatted_code = \"clf_predictor = clf_estimator.deploy(\\n    initial_instance_count=1,\\n    instance_type=infer_instance_type,\\n    endpoint_name=endpoint_name,\\n    model_name=model_name,\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will take around 10-15 minutes\n",
    "clf_predictor = clf_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=infer_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f13275ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 32;\n                var nbb_unformatted_code = \"sgmkr_runt = boto3.client(\\\"runtime.sagemaker\\\")\";\n                var nbb_formatted_code = \"sgmkr_runt = boto3.client(\\\"runtime.sagemaker\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgmkr_runt = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa70b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14996729791164398, 0.8495358228683472, 0.0004969232250005007]\n",
      "The predicted flower is: rose\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 39;\n                var nbb_unformatted_code = \"with open(\\\"images/flowers/rose/1645761726_2b1be95472.jpg\\\", \\\"rb\\\") as image:\\n        payload = image.read()\\n        payload = bytearray(payload)\\n        \\nresponse = sgmkr_runt.invoke_endpoint(\\n    EndpointName = endpoint_name,\\n    ContentType = 'image/jpeg',\\n    Accept='application/json',  # Specify proper accept header\\n    Body = payload,\\n)\\n\\nprediction = json.loads(response['Body'].read().decode())\\nprint(prediction)\\npredicted_class = subfolders[prediction.index(max(prediction))]\\nprint(f\\\"The predicted flower is: {predicted_class}\\\")\";\n                var nbb_formatted_code = \"with open(\\\"images/flowers/rose/1645761726_2b1be95472.jpg\\\", \\\"rb\\\") as image:\\n    payload = image.read()\\n    payload = bytearray(payload)\\n\\nresponse = sgmkr_runt.invoke_endpoint(\\n    EndpointName=endpoint_name,\\n    ContentType=\\\"image/jpeg\\\",\\n    Accept=\\\"application/json\\\",  # Specify proper accept header\\n    Body=payload,\\n)\\n\\nprediction = json.loads(response[\\\"Body\\\"].read().decode())\\nprint(prediction)\\npredicted_class = subfolders[prediction.index(max(prediction))]\\nprint(f\\\"The predicted flower is: {predicted_class}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"images/flowers/rose/1645761726_2b1be95472.jpg\", \"rb\") as image:\n",
    "        payload = image.read()\n",
    "        payload = bytearray(payload)\n",
    "        \n",
    "response = sgmkr_runt.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = 'image/jpeg',\n",
    "    Accept='application/json',  # Specify proper accept header\n",
    "    Body = payload,\n",
    ")\n",
    "\n",
    "prediction = json.loads(response['Body'].read().decode())\n",
    "print(prediction)\n",
    "predicted_class = subfolders[prediction.index(max(prediction))]\n",
    "print(f\"The predicted flower is: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec51f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/12/25 18:25:45] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4865\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4865</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/12/25 18:25:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=282825;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=141786;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4865\u001b\\\u001b[2m4865\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m                              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name: flowers-clf-ib-resent-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4855\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4855</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name: flowers-clf-ib-resent-\u001b[1;36m2025\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m17\u001b[0m-\u001b[1;36m57\u001b[0m-\u001b[1;36m19\u001b[0m \u001b]8;id=742286;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=172596;file:///home/thangtran3112/aws-sage-maker/machine-learning/venv/lib/python3.12/site-packages/sagemaker/session.py#4855\u001b\\\u001b[2m4855\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 40;\n                var nbb_unformatted_code = \"clf_predictor.delete_endpoint()\";\n                var nbb_formatted_code = \"clf_predictor.delete_endpoint()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
