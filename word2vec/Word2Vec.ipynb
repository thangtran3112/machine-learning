{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 17:52:13,621 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
      "2024-11-27 17:52:13,630 : INFO : read 0 reviews\n",
      "2024-11-27 17:52:14,708 : INFO : read 10000 reviews\n",
      "2024-11-27 17:52:15,832 : INFO : read 20000 reviews\n",
      "2024-11-27 17:52:17,108 : INFO : read 30000 reviews\n",
      "2024-11-27 17:52:18,276 : INFO : read 40000 reviews\n",
      "2024-11-27 17:52:19,689 : INFO : read 50000 reviews\n",
      "2024-11-27 17:52:20,945 : INFO : read 60000 reviews\n",
      "2024-11-27 17:52:22,043 : INFO : read 70000 reviews\n",
      "2024-11-27 17:52:23,007 : INFO : read 80000 reviews\n",
      "2024-11-27 17:52:24,054 : INFO : read 90000 reviews\n",
      "2024-11-27 17:52:25,042 : INFO : read 100000 reviews\n",
      "2024-11-27 17:52:26,019 : INFO : read 110000 reviews\n",
      "2024-11-27 17:52:27,010 : INFO : read 120000 reviews\n",
      "2024-11-27 17:52:28,063 : INFO : read 130000 reviews\n",
      "2024-11-27 17:52:29,154 : INFO : read 140000 reviews\n",
      "2024-11-27 17:52:30,422 : INFO : read 150000 reviews\n",
      "2024-11-27 17:52:31,448 : INFO : read 160000 reviews\n",
      "2024-11-27 17:52:32,452 : INFO : read 170000 reviews\n",
      "2024-11-27 17:52:33,538 : INFO : read 180000 reviews\n",
      "2024-11-27 17:52:34,576 : INFO : read 190000 reviews\n",
      "2024-11-27 17:52:35,713 : INFO : read 200000 reviews\n",
      "2024-11-27 17:52:36,781 : INFO : read 210000 reviews\n",
      "2024-11-27 17:52:37,867 : INFO : read 220000 reviews\n",
      "2024-11-27 17:52:38,858 : INFO : read 230000 reviews\n",
      "2024-11-27 17:52:40,433 : INFO : read 240000 reviews\n",
      "2024-11-27 17:52:41,495 : INFO : read 250000 reviews\n",
      "2024-11-27 17:52:42,051 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # lowercase, remove special characters, punctuations, numbers. Also tokenizes the text into a list of words.\n",
    "            # The yield statement turns the function into a generator, allowing the caller to process one line at a time.\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 17:52:45,069 : INFO : collecting all words and their counts\n",
      "2024-11-27 17:52:45,070 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-11-27 17:52:45,250 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2024-11-27 17:52:45,460 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2024-11-27 17:52:45,647 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2024-11-27 17:52:45,815 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2024-11-27 17:52:46,001 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2024-11-27 17:52:46,180 : INFO : PROGRESS: at sentence #60000, processed 11013728 words, keeping 76788 word types\n",
      "2024-11-27 17:52:46,341 : INFO : PROGRESS: at sentence #70000, processed 12637530 words, keeping 83201 word types\n",
      "2024-11-27 17:52:46,484 : INFO : PROGRESS: at sentence #80000, processed 14099756 words, keeping 88461 word types\n",
      "2024-11-27 17:52:46,632 : INFO : PROGRESS: at sentence #90000, processed 15662154 words, keeping 93359 word types\n",
      "2024-11-27 17:52:46,772 : INFO : PROGRESS: at sentence #100000, processed 17164492 words, keeping 97888 word types\n",
      "2024-11-27 17:52:46,909 : INFO : PROGRESS: at sentence #110000, processed 18652297 words, keeping 102134 word types\n",
      "2024-11-27 17:52:47,048 : INFO : PROGRESS: at sentence #120000, processed 20152534 words, keeping 105925 word types\n",
      "2024-11-27 17:52:47,205 : INFO : PROGRESS: at sentence #130000, processed 21684335 words, keeping 110106 word types\n",
      "2024-11-27 17:52:47,366 : INFO : PROGRESS: at sentence #140000, processed 23330211 words, keeping 114110 word types\n",
      "2024-11-27 17:52:47,509 : INFO : PROGRESS: at sentence #150000, processed 24838759 words, keeping 118176 word types\n",
      "2024-11-27 17:52:47,655 : INFO : PROGRESS: at sentence #160000, processed 26390915 words, keeping 118672 word types\n",
      "2024-11-27 17:52:47,802 : INFO : PROGRESS: at sentence #170000, processed 27913921 words, keeping 123358 word types\n",
      "2024-11-27 17:52:47,961 : INFO : PROGRESS: at sentence #180000, processed 29535617 words, keeping 126750 word types\n",
      "2024-11-27 17:52:48,112 : INFO : PROGRESS: at sentence #190000, processed 31096464 words, keeping 129849 word types\n",
      "2024-11-27 17:52:48,273 : INFO : PROGRESS: at sentence #200000, processed 32805276 words, keeping 133257 word types\n",
      "2024-11-27 17:52:48,424 : INFO : PROGRESS: at sentence #210000, processed 34434203 words, keeping 136366 word types\n",
      "2024-11-27 17:52:48,583 : INFO : PROGRESS: at sentence #220000, processed 36083487 words, keeping 139420 word types\n",
      "2024-11-27 17:52:48,723 : INFO : PROGRESS: at sentence #230000, processed 37571767 words, keeping 142401 word types\n",
      "2024-11-27 17:52:48,872 : INFO : PROGRESS: at sentence #240000, processed 39138195 words, keeping 145234 word types\n",
      "2024-11-27 17:52:49,021 : INFO : PROGRESS: at sentence #250000, processed 40695054 words, keeping 147968 word types\n",
      "2024-11-27 17:52:49,106 : INFO : collected 150061 word types from a corpus of 41519360 raw words and 255404 sentences\n",
      "2024-11-27 17:52:49,106 : INFO : Creating a fresh vocabulary\n",
      "2024-11-27 17:52:49,239 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 70537 unique words (47.01% of original 150061, drops 79524)', 'datetime': '2024-11-27T17:52:49.239754', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-27 17:52:49,240 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 41439836 word corpus (99.81% of original 41519360, drops 79524)', 'datetime': '2024-11-27T17:52:49.240367', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-27 17:52:49,409 : INFO : deleting the raw counts dictionary of 150061 items\n",
      "2024-11-27 17:52:49,411 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2024-11-27 17:52:49,412 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 30349251.36700416 word corpus (73.2%% of prior 41439836)', 'datetime': '2024-11-27T17:52:49.412185', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-11-27 17:52:49,681 : INFO : estimated required memory for 70537 words and 150 dimensions: 119912900 bytes\n",
      "2024-11-27 17:52:49,682 : INFO : resetting layer weights\n",
      "2024-11-27 17:52:49,714 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-27T17:52:49.714609', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-11-27 17:52:49,715 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-11-27T17:52:49.715039', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-11-27 17:52:50,725 : INFO : EPOCH 0 - PROGRESS: at 7.97% examples, 2472173 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:52:51,726 : INFO : EPOCH 0 - PROGRESS: at 14.70% examples, 2428533 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:52:52,729 : INFO : EPOCH 0 - PROGRESS: at 20.96% examples, 2395162 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:52:53,729 : INFO : EPOCH 0 - PROGRESS: at 29.05% examples, 2416997 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:52:54,730 : INFO : EPOCH 0 - PROGRESS: at 38.38% examples, 2465834 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:52:55,734 : INFO : EPOCH 0 - PROGRESS: at 47.72% examples, 2486894 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:52:56,734 : INFO : EPOCH 0 - PROGRESS: at 56.46% examples, 2496846 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:52:57,736 : INFO : EPOCH 0 - PROGRESS: at 65.11% examples, 2490418 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:52:58,737 : INFO : EPOCH 0 - PROGRESS: at 74.10% examples, 2512522 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:52:59,740 : INFO : EPOCH 0 - PROGRESS: at 82.82% examples, 2530412 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:00,742 : INFO : EPOCH 0 - PROGRESS: at 91.65% examples, 2534562 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:01,668 : INFO : EPOCH 0: training on 41519360 raw words (30349920 effective words) took 11.9s, 2541190 effective words/s\n",
      "2024-11-27 17:53:02,677 : INFO : EPOCH 1 - PROGRESS: at 8.57% examples, 2640399 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:03,677 : INFO : EPOCH 1 - PROGRESS: at 15.69% examples, 2585693 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:04,684 : INFO : EPOCH 1 - PROGRESS: at 22.69% examples, 2567489 words/s, in_qsize 16, out_qsize 3\n",
      "2024-11-27 17:53:05,688 : INFO : EPOCH 1 - PROGRESS: at 31.82% examples, 2599406 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:06,693 : INFO : EPOCH 1 - PROGRESS: at 41.28% examples, 2616220 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:07,701 : INFO : EPOCH 1 - PROGRESS: at 50.80% examples, 2624620 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:08,703 : INFO : EPOCH 1 - PROGRESS: at 59.23% examples, 2603636 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:09,705 : INFO : EPOCH 1 - PROGRESS: at 68.07% examples, 2595676 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:10,713 : INFO : EPOCH 1 - PROGRESS: at 76.72% examples, 2597254 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:11,714 : INFO : EPOCH 1 - PROGRESS: at 85.33% examples, 2603352 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:12,719 : INFO : EPOCH 1 - PROGRESS: at 94.31% examples, 2598464 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:13,319 : INFO : EPOCH 1: training on 41519360 raw words (30352646 effective words) took 11.6s, 2606007 effective words/s\n",
      "2024-11-27 17:53:14,325 : INFO : EPOCH 2 - PROGRESS: at 8.53% examples, 2632450 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:15,326 : INFO : EPOCH 2 - PROGRESS: at 15.84% examples, 2621929 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:16,328 : INFO : EPOCH 2 - PROGRESS: at 23.07% examples, 2624334 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:17,333 : INFO : EPOCH 2 - PROGRESS: at 31.92% examples, 2610469 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:18,335 : INFO : EPOCH 2 - PROGRESS: at 41.30% examples, 2622183 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:19,336 : INFO : EPOCH 2 - PROGRESS: at 50.41% examples, 2613191 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:20,339 : INFO : EPOCH 2 - PROGRESS: at 59.65% examples, 2625537 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:21,369 : INFO : EPOCH 2 - PROGRESS: at 68.26% examples, 2598469 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:22,373 : INFO : EPOCH 2 - PROGRESS: at 76.80% examples, 2597004 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:23,377 : INFO : EPOCH 2 - PROGRESS: at 85.64% examples, 2609221 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:24,379 : INFO : EPOCH 2 - PROGRESS: at 95.04% examples, 2615913 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:24,944 : INFO : EPOCH 2: training on 41519360 raw words (30351112 effective words) took 11.6s, 2611735 effective words/s\n",
      "2024-11-27 17:53:25,958 : INFO : EPOCH 3 - PROGRESS: at 8.60% examples, 2635430 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:26,958 : INFO : EPOCH 3 - PROGRESS: at 16.26% examples, 2688026 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:27,958 : INFO : EPOCH 3 - PROGRESS: at 23.25% examples, 2643930 words/s, in_qsize 19, out_qsize 1\n",
      "2024-11-27 17:53:28,962 : INFO : EPOCH 3 - PROGRESS: at 32.22% examples, 2630631 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:29,965 : INFO : EPOCH 3 - PROGRESS: at 41.63% examples, 2635393 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:30,967 : INFO : EPOCH 3 - PROGRESS: at 51.06% examples, 2642257 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:31,969 : INFO : EPOCH 3 - PROGRESS: at 60.29% examples, 2651690 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:32,979 : INFO : EPOCH 3 - PROGRESS: at 69.37% examples, 2645400 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:33,980 : INFO : EPOCH 3 - PROGRESS: at 77.55% examples, 2628851 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:34,983 : INFO : EPOCH 3 - PROGRESS: at 86.33% examples, 2633485 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:35,984 : INFO : EPOCH 3 - PROGRESS: at 95.73% examples, 2638138 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:36,431 : INFO : EPOCH 3: training on 41519360 raw words (30346921 effective words) took 11.5s, 2642937 effective words/s\n",
      "2024-11-27 17:53:37,438 : INFO : EPOCH 4 - PROGRESS: at 8.48% examples, 2617922 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:38,438 : INFO : EPOCH 4 - PROGRESS: at 15.80% examples, 2614908 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:39,441 : INFO : EPOCH 4 - PROGRESS: at 22.85% examples, 2589962 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:40,441 : INFO : EPOCH 4 - PROGRESS: at 31.94% examples, 2614689 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:53:41,448 : INFO : EPOCH 4 - PROGRESS: at 40.76% examples, 2595933 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:42,451 : INFO : EPOCH 4 - PROGRESS: at 49.88% examples, 2587019 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:43,457 : INFO : EPOCH 4 - PROGRESS: at 59.14% examples, 2602786 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:44,463 : INFO : EPOCH 4 - PROGRESS: at 68.47% examples, 2612327 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:53:45,467 : INFO : EPOCH 4 - PROGRESS: at 76.85% examples, 2603666 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:46,468 : INFO : EPOCH 4 - PROGRESS: at 85.13% examples, 2599072 words/s, in_qsize 18, out_qsize 2\n",
      "2024-11-27 17:53:47,472 : INFO : EPOCH 4 - PROGRESS: at 94.73% examples, 2612537 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:53:48,018 : INFO : EPOCH 4: training on 41519360 raw words (30349895 effective words) took 11.6s, 2620285 effective words/s\n",
      "2024-11-27 17:53:48,019 : INFO : Word2Vec lifecycle event {'msg': 'training on 207596800 raw words (151750494 effective words) took 58.3s, 2602762 effective words/s', 'datetime': '2024-11-27T17:53:48.019265', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-11-27 17:53:48,019 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=70537, vector_size=150, alpha=0.025>', 'datetime': '2024-11-27T17:53:48.019560', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-11-27 17:53:48,020 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2024-11-27 17:53:48,020 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-11-27T17:53:48.020230', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-11-27 17:53:49,029 : INFO : EPOCH 0 - PROGRESS: at 8.38% examples, 2581505 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:50,034 : INFO : EPOCH 0 - PROGRESS: at 15.97% examples, 2636911 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:51,037 : INFO : EPOCH 0 - PROGRESS: at 23.10% examples, 2622259 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:52,040 : INFO : EPOCH 0 - PROGRESS: at 32.12% examples, 2622232 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:53,044 : INFO : EPOCH 0 - PROGRESS: at 41.80% examples, 2642876 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:54,049 : INFO : EPOCH 0 - PROGRESS: at 51.12% examples, 2644303 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:55,049 : INFO : EPOCH 0 - PROGRESS: at 60.30% examples, 2650079 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:56,049 : INFO : EPOCH 0 - PROGRESS: at 69.12% examples, 2637473 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:57,056 : INFO : EPOCH 0 - PROGRESS: at 77.99% examples, 2645000 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:58,061 : INFO : EPOCH 0 - PROGRESS: at 85.28% examples, 2603205 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:53:59,066 : INFO : EPOCH 0 - PROGRESS: at 94.33% examples, 2600313 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:53:59,657 : INFO : EPOCH 0: training on 41519360 raw words (30351278 effective words) took 11.6s, 2609081 effective words/s\n",
      "2024-11-27 17:54:00,675 : INFO : EPOCH 1 - PROGRESS: at 7.84% examples, 2425593 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:01,677 : INFO : EPOCH 1 - PROGRESS: at 15.09% examples, 2497202 words/s, in_qsize 16, out_qsize 3\n",
      "2024-11-27 17:54:02,677 : INFO : EPOCH 1 - PROGRESS: at 22.45% examples, 2540583 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:03,681 : INFO : EPOCH 1 - PROGRESS: at 30.85% examples, 2542861 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:04,683 : INFO : EPOCH 1 - PROGRESS: at 40.16% examples, 2561395 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:54:05,689 : INFO : EPOCH 1 - PROGRESS: at 49.59% examples, 2572505 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:06,691 : INFO : EPOCH 1 - PROGRESS: at 58.32% examples, 2570800 words/s, in_qsize 20, out_qsize 2\n",
      "2024-11-27 17:54:07,694 : INFO : EPOCH 1 - PROGRESS: at 67.71% examples, 2585579 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:08,698 : INFO : EPOCH 1 - PROGRESS: at 76.21% examples, 2583789 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:09,707 : INFO : EPOCH 1 - PROGRESS: at 83.40% examples, 2543304 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:10,709 : INFO : EPOCH 1 - PROGRESS: at 91.19% examples, 2517580 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:11,725 : INFO : EPOCH 1 - PROGRESS: at 99.46% examples, 2505856 words/s, in_qsize 15, out_qsize 4\n",
      "2024-11-27 17:54:11,772 : INFO : EPOCH 1: training on 41519360 raw words (30345565 effective words) took 12.1s, 2508257 effective words/s\n",
      "2024-11-27 17:54:12,779 : INFO : EPOCH 2 - PROGRESS: at 8.04% examples, 2485823 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:13,781 : INFO : EPOCH 2 - PROGRESS: at 15.84% examples, 2618449 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:54:14,782 : INFO : EPOCH 2 - PROGRESS: at 22.83% examples, 2586611 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:15,785 : INFO : EPOCH 2 - PROGRESS: at 30.79% examples, 2538813 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:16,788 : INFO : EPOCH 2 - PROGRESS: at 40.01% examples, 2554136 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:17,788 : INFO : EPOCH 2 - PROGRESS: at 49.78% examples, 2583156 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:18,789 : INFO : EPOCH 2 - PROGRESS: at 58.29% examples, 2571388 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:19,790 : INFO : EPOCH 2 - PROGRESS: at 67.26% examples, 2571599 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:54:20,793 : INFO : EPOCH 2 - PROGRESS: at 75.61% examples, 2563669 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:21,796 : INFO : EPOCH 2 - PROGRESS: at 83.49% examples, 2549820 words/s, in_qsize 20, out_qsize 3\n",
      "2024-11-27 17:54:22,797 : INFO : EPOCH 2 - PROGRESS: at 92.33% examples, 2550648 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:23,616 : INFO : EPOCH 2: training on 41519360 raw words (30349350 effective words) took 11.8s, 2563263 effective words/s\n",
      "2024-11-27 17:54:24,621 : INFO : EPOCH 3 - PROGRESS: at 8.36% examples, 2584183 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:25,625 : INFO : EPOCH 3 - PROGRESS: at 15.25% examples, 2521104 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:26,627 : INFO : EPOCH 3 - PROGRESS: at 22.11% examples, 2497428 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:27,628 : INFO : EPOCH 3 - PROGRESS: at 30.09% examples, 2492365 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:28,631 : INFO : EPOCH 3 - PROGRESS: at 39.51% examples, 2526453 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:29,632 : INFO : EPOCH 3 - PROGRESS: at 49.22% examples, 2555162 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:30,632 : INFO : EPOCH 3 - PROGRESS: at 58.15% examples, 2565791 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:31,634 : INFO : EPOCH 3 - PROGRESS: at 67.30% examples, 2572687 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:54:32,635 : INFO : EPOCH 3 - PROGRESS: at 76.28% examples, 2588947 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:33,635 : INFO : EPOCH 3 - PROGRESS: at 84.63% examples, 2588272 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:34,643 : INFO : EPOCH 3 - PROGRESS: at 93.79% examples, 2590513 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:35,319 : INFO : EPOCH 3: training on 41519360 raw words (30347313 effective words) took 11.7s, 2594133 effective words/s\n",
      "2024-11-27 17:54:36,329 : INFO : EPOCH 4 - PROGRESS: at 8.41% examples, 2585838 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:37,333 : INFO : EPOCH 4 - PROGRESS: at 15.99% examples, 2640142 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:38,335 : INFO : EPOCH 4 - PROGRESS: at 23.33% examples, 2651037 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:54:39,345 : INFO : EPOCH 4 - PROGRESS: at 31.88% examples, 2598493 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:40,346 : INFO : EPOCH 4 - PROGRESS: at 40.72% examples, 2588767 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:41,348 : INFO : EPOCH 4 - PROGRESS: at 50.25% examples, 2599481 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:42,350 : INFO : EPOCH 4 - PROGRESS: at 59.23% examples, 2604990 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:43,352 : INFO : EPOCH 4 - PROGRESS: at 68.26% examples, 2603921 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:44,353 : INFO : EPOCH 4 - PROGRESS: at 77.03% examples, 2610307 words/s, in_qsize 20, out_qsize 2\n",
      "2024-11-27 17:54:45,356 : INFO : EPOCH 4 - PROGRESS: at 85.31% examples, 2604594 words/s, in_qsize 19, out_qsize 1\n",
      "2024-11-27 17:54:46,357 : INFO : EPOCH 4 - PROGRESS: at 94.28% examples, 2600513 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:47,012 : INFO : EPOCH 4: training on 41519360 raw words (30350333 effective words) took 11.7s, 2596320 effective words/s\n",
      "2024-11-27 17:54:48,022 : INFO : EPOCH 5 - PROGRESS: at 8.56% examples, 2639601 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:54:49,026 : INFO : EPOCH 5 - PROGRESS: at 16.11% examples, 2668000 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:50,027 : INFO : EPOCH 5 - PROGRESS: at 23.45% examples, 2672421 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:51,028 : INFO : EPOCH 5 - PROGRESS: at 32.27% examples, 2638098 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:54:52,029 : INFO : EPOCH 5 - PROGRESS: at 40.60% examples, 2589123 words/s, in_qsize 20, out_qsize 2\n",
      "2024-11-27 17:54:53,035 : INFO : EPOCH 5 - PROGRESS: at 49.53% examples, 2569631 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:54:54,039 : INFO : EPOCH 5 - PROGRESS: at 58.51% examples, 2578762 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:55,040 : INFO : EPOCH 5 - PROGRESS: at 67.63% examples, 2583431 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:54:56,040 : INFO : EPOCH 5 - PROGRESS: at 76.22% examples, 2585233 words/s, in_qsize 20, out_qsize 1\n",
      "2024-11-27 17:54:57,041 : INFO : EPOCH 5 - PROGRESS: at 84.49% examples, 2581860 words/s, in_qsize 19, out_qsize 1\n",
      "2024-11-27 17:54:58,042 : INFO : EPOCH 5 - PROGRESS: at 93.12% examples, 2573156 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:54:58,772 : INFO : EPOCH 5: training on 41519360 raw words (30349702 effective words) took 11.8s, 2582597 effective words/s\n",
      "2024-11-27 17:54:59,777 : INFO : EPOCH 6 - PROGRESS: at 8.12% examples, 2512113 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:00,778 : INFO : EPOCH 6 - PROGRESS: at 15.73% examples, 2596327 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:01,780 : INFO : EPOCH 6 - PROGRESS: at 22.99% examples, 2612552 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:02,793 : INFO : EPOCH 6 - PROGRESS: at 32.08% examples, 2619168 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:03,795 : INFO : EPOCH 6 - PROGRESS: at 40.47% examples, 2575938 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:04,799 : INFO : EPOCH 6 - PROGRESS: at 50.00% examples, 2588073 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:05,806 : INFO : EPOCH 6 - PROGRESS: at 59.08% examples, 2595995 words/s, in_qsize 17, out_qsize 2\n",
      "2024-11-27 17:55:06,806 : INFO : EPOCH 6 - PROGRESS: at 68.19% examples, 2600521 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:07,810 : INFO : EPOCH 6 - PROGRESS: at 76.72% examples, 2598314 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:08,814 : INFO : EPOCH 6 - PROGRESS: at 84.95% examples, 2592312 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:09,814 : INFO : EPOCH 6 - PROGRESS: at 93.87% examples, 2588779 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:55:10,487 : INFO : EPOCH 6: training on 41519360 raw words (30346528 effective words) took 11.7s, 2591291 effective words/s\n",
      "2024-11-27 17:55:11,493 : INFO : EPOCH 7 - PROGRESS: at 8.51% examples, 2624559 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:12,500 : INFO : EPOCH 7 - PROGRESS: at 15.03% examples, 2479300 words/s, in_qsize 15, out_qsize 4\n",
      "2024-11-27 17:55:13,502 : INFO : EPOCH 7 - PROGRESS: at 22.48% examples, 2539277 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:14,503 : INFO : EPOCH 7 - PROGRESS: at 31.32% examples, 2568936 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:15,503 : INFO : EPOCH 7 - PROGRESS: at 40.56% examples, 2584203 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:16,507 : INFO : EPOCH 7 - PROGRESS: at 49.68% examples, 2576796 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:17,509 : INFO : EPOCH 7 - PROGRESS: at 58.27% examples, 2568631 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:18,520 : INFO : EPOCH 7 - PROGRESS: at 67.48% examples, 2574661 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:19,526 : INFO : EPOCH 7 - PROGRESS: at 76.03% examples, 2573194 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:20,536 : INFO : EPOCH 7 - PROGRESS: at 83.30% examples, 2537429 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:21,537 : INFO : EPOCH 7 - PROGRESS: at 90.65% examples, 2501406 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:55:22,565 : INFO : EPOCH 7 - PROGRESS: at 97.41% examples, 2451339 words/s, in_qsize 19, out_qsize 6\n",
      "2024-11-27 17:55:22,885 : INFO : EPOCH 7: training on 41519360 raw words (30345217 effective words) took 12.4s, 2448368 effective words/s\n",
      "2024-11-27 17:55:23,916 : INFO : EPOCH 8 - PROGRESS: at 8.02% examples, 2469427 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:24,920 : INFO : EPOCH 8 - PROGRESS: at 15.51% examples, 2553018 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:25,926 : INFO : EPOCH 8 - PROGRESS: at 23.07% examples, 2613829 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:26,928 : INFO : EPOCH 8 - PROGRESS: at 32.47% examples, 2645066 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:55:27,934 : INFO : EPOCH 8 - PROGRESS: at 41.79% examples, 2639693 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:28,939 : INFO : EPOCH 8 - PROGRESS: at 50.90% examples, 2628748 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:29,941 : INFO : EPOCH 8 - PROGRESS: at 60.12% examples, 2639944 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:30,943 : INFO : EPOCH 8 - PROGRESS: at 69.61% examples, 2654330 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:31,946 : INFO : EPOCH 8 - PROGRESS: at 78.39% examples, 2656922 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:32,947 : INFO : EPOCH 8 - PROGRESS: at 86.91% examples, 2648534 words/s, in_qsize 20, out_qsize 0\n",
      "2024-11-27 17:55:33,951 : INFO : EPOCH 8 - PROGRESS: at 96.11% examples, 2647768 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:34,351 : INFO : EPOCH 8: training on 41519360 raw words (30348096 effective words) took 11.4s, 2652367 effective words/s\n",
      "2024-11-27 17:55:35,362 : INFO : EPOCH 9 - PROGRESS: at 8.67% examples, 2661725 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:36,367 : INFO : EPOCH 9 - PROGRESS: at 16.09% examples, 2656501 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:37,373 : INFO : EPOCH 9 - PROGRESS: at 23.66% examples, 2688777 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:38,375 : INFO : EPOCH 9 - PROGRESS: at 33.33% examples, 2705145 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:39,380 : INFO : EPOCH 9 - PROGRESS: at 43.11% examples, 2715589 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:40,384 : INFO : EPOCH 9 - PROGRESS: at 52.41% examples, 2705076 words/s, in_qsize 16, out_qsize 3\n",
      "2024-11-27 17:55:41,388 : INFO : EPOCH 9 - PROGRESS: at 61.62% examples, 2701481 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:42,393 : INFO : EPOCH 9 - PROGRESS: at 70.79% examples, 2698734 words/s, in_qsize 18, out_qsize 1\n",
      "2024-11-27 17:55:43,399 : INFO : EPOCH 9 - PROGRESS: at 79.82% examples, 2702768 words/s, in_qsize 16, out_qsize 3\n",
      "2024-11-27 17:55:44,411 : INFO : EPOCH 9 - PROGRESS: at 88.91% examples, 2698396 words/s, in_qsize 19, out_qsize 0\n",
      "2024-11-27 17:55:45,415 : INFO : EPOCH 9 - PROGRESS: at 98.11% examples, 2695512 words/s, in_qsize 20, out_qsize 2\n",
      "2024-11-27 17:55:45,595 : INFO : EPOCH 9: training on 41519360 raw words (30350239 effective words) took 11.2s, 2700333 effective words/s\n",
      "2024-11-27 17:55:45,595 : INFO : Word2Vec lifecycle event {'msg': 'training on 415193600 raw words (303483621 effective words) took 117.6s, 2581199 effective words/s', 'datetime': '2024-11-27T17:55:45.595738', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303483621, 415193600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 features vector, \n",
    "# window=10: maximum distance between the target words and context words to consider during training\n",
    "# workers = 10, 10 threads\n",
    "# min_count=2, Ignores words that appear fewer than 2 times in the corpus\n",
    "model = gensim.models.Word2Vec (documents, window=10, min_count=2, workers=10, vector_size=100)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8634127974510193),\n",
       " ('stained', 0.7752071022987366),\n",
       " ('unclean', 0.7722263336181641),\n",
       " ('smelly', 0.7663547396659851),\n",
       " ('dusty', 0.7622085213661194),\n",
       " ('grubby', 0.742056667804718),\n",
       " ('dingy', 0.7373906970024109),\n",
       " ('disgusting', 0.7170394062995911),\n",
       " ('grimy', 0.709922194480896),\n",
       " ('mouldy', 0.7083764672279358)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9191972017288208),\n",
       " ('friendly', 0.8196632862091064),\n",
       " ('curteous', 0.7884572744369507),\n",
       " ('professional', 0.7882914543151855),\n",
       " ('cordial', 0.784886360168457),\n",
       " ('attentive', 0.7762936949729919)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canada', 0.6823714971542358),\n",
       " ('germany', 0.6659719347953796),\n",
       " ('spain', 0.6570398211479187),\n",
       " ('mexico', 0.6134476065635681),\n",
       " ('hawaii', 0.6121428608894348),\n",
       " ('england', 0.6112246513366699)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.7981869578361511),\n",
       " ('amazed', 0.781319260597229),\n",
       " ('dismayed', 0.7661396265029907),\n",
       " ('astonished', 0.763482928276062),\n",
       " ('appalled', 0.7575596570968628),\n",
       " ('stunned', 0.7556308507919312)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mattress', 0.7134687900543213),\n",
       " ('blanket', 0.6933295130729675),\n",
       " ('duvet', 0.6915439963340759),\n",
       " ('matress', 0.6841074228286743),\n",
       " ('quilt', 0.6617224216461182),\n",
       " ('pillowcase', 0.6444460153579712),\n",
       " ('sheets', 0.6385989785194397),\n",
       " ('pillows', 0.629342794418335),\n",
       " ('foam', 0.6265518665313721),\n",
       " ('bolster', 0.6236247420310974)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77996206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25355593501920781"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `vector_size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
